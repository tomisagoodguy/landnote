{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 16:22:09,927 - INFO - 開始執行文章爬蟲\n",
      "2025-02-02 16:22:09,927 - INFO - 開始執行文章爬蟲\n",
      "2025-02-02 16:22:10,434 - INFO - 找到 1 頁期刊\n",
      "2025-02-02 16:22:10,434 - INFO - 找到 1 頁期刊\n",
      "2025-02-02 16:22:11,481 - INFO - 從期刊頁面找到 0 篇文章\n",
      "2025-02-02 16:22:11,481 - INFO - 從期刊頁面找到 0 篇文章\n",
      "處理文章批次: 0it [00:00, ?it/s]\n",
      "2025-02-02 16:22:11,482 - INFO - 文章爬蟲執行完成\n",
      "2025-02-02 16:22:11,482 - INFO - 文章爬蟲執行完成\n"
     ]
    }
   ],
   "source": [
    "# 導入所需套件\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "import urllib3\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class ArticleScraper:\n",
    "    def __init__(self, scan_mode=\"all\", data_file=\"articles.xlsx\"):\n",
    "        \"\"\"初始化爬蟲設定\"\"\"\n",
    "        # 基本設定\n",
    "        self.base_url = \"https://real-estate.get.com.tw/Columns/\"\n",
    "        self.detail_url = f\"{self.base_url}detail.aspx\"\n",
    "        self.journal_url = f\"{self.base_url}journal.aspx\"\n",
    "        self.target_authors = [\"曾榮耀\", \"許文昌\", \"蘇偉強\"]\n",
    "        self.scan_mode = scan_mode\n",
    "        self.data_file = Path(data_file)\n",
    "\n",
    "        # 期刊參數設定\n",
    "        self.journal_params = {\n",
    "            \"no\": \"1282\",\n",
    "            \"pno\": \"51121\"\n",
    "        }\n",
    "\n",
    "        # 時間範圍設定\n",
    "        self.end_date = datetime.now()\n",
    "        self.start_date = self.end_date - timedelta(days=9*365)\n",
    "\n",
    "        # 效能設定\n",
    "        self.batch_size = 50\n",
    "        self.max_workers = 4\n",
    "        self.max_retries = 5\n",
    "        self.retry_delay = 3\n",
    "        self.request_interval = 1.5\n",
    "\n",
    "        # 文章編號範圍\n",
    "        self.start_no = 900000\n",
    "        self.max_no = 915000\n",
    "\n",
    "        # 初始化\n",
    "        self.setup_directories()\n",
    "        self.setup_session()\n",
    "        self.setup_logger()\n",
    "        self.processed_articles = set()\n",
    "        self.load_processed_articles()\n",
    "        self.last_request_time = 0\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"建立必要的目錄結構\"\"\"\n",
    "        self.base_dir = Path(\"real_estate_articles\")\n",
    "        self.articles_dir = self.base_dir / \"articles\"\n",
    "        self.images_dir = self.articles_dir / \"images\"\n",
    "        self.logs_dir = self.base_dir / \"logs\"\n",
    "\n",
    "        for directory in [self.base_dir, self.articles_dir, self.images_dir, self.logs_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 創建預設的失敗圖片\n",
    "        self.failed_image_path = self.images_dir / \"image_download_failed.png\"\n",
    "        if not self.failed_image_path.exists():\n",
    "            try:\n",
    "                from PIL import Image, ImageDraw\n",
    "                img = Image.new('RGB', (400, 100), color='white')\n",
    "                d = ImageDraw.Draw(img)\n",
    "                d.text((10, 40), \"Image Download Failed\", fill='black')\n",
    "                img.save(self.failed_image_path)\n",
    "            except Exception:\n",
    "                self.failed_image_path.touch()\n",
    "\n",
    "    def setup_session(self):\n",
    "        \"\"\"設定請求session\"\"\"\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        })\n",
    "\n",
    "    def setup_logger(self):\n",
    "        \"\"\"設定日誌系統\"\"\"\n",
    "        self.logger = logging.getLogger('ArticleScraper')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        log_file = self.logs_dir / f\"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        handlers = [\n",
    "            logging.FileHandler(log_file, encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        for handler in handlers:\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def load_processed_articles(self):\n",
    "        \"\"\"載入已處理的文章\"\"\"\n",
    "        if self.data_file.exists():\n",
    "            df = pd.read_excel(self.data_file)\n",
    "            if '文章編號' in df.columns:\n",
    "                self.processed_articles = set(df['文章編號'].astype(str))\n",
    "\n",
    "    def wait_between_requests(self):\n",
    "        \"\"\"控制請求間隔\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_request_time\n",
    "        if elapsed < self.request_interval:\n",
    "            time.sleep(self.request_interval - elapsed)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def get_max_page_number(self) -> int:\n",
    "        \"\"\"獲取期刊最大頁數\"\"\"\n",
    "        try:\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=self.journal_params,\n",
    "                timeout=30,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 找到分頁元素\n",
    "            pagination = soup.select('.pagination a')\n",
    "            if pagination:\n",
    "                page_numbers = []\n",
    "                for a in pagination:\n",
    "                    try:\n",
    "                        page_numbers.append(int(a.text.strip()))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                return max(page_numbers) if page_numbers else 1\n",
    "            return 1\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取最大頁數失敗: {str(e)}\")\n",
    "            return 30  # 預設較大的頁數以確保不遺漏\n",
    "\n",
    "    def get_article_urls_from_journal(self, page_no: int) -> List[int]:\n",
    "        \"\"\"從期刊頁面獲取文章編號列表\"\"\"\n",
    "        try:\n",
    "            params = self.journal_params.copy()\n",
    "            params['page_no'] = page_no\n",
    "\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=params,\n",
    "                timeout=30,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            articles = []\n",
    "            for link in soup.select('a[href*=\"detail.aspx?no=\"]'):\n",
    "                href = link.get('href', '')\n",
    "                if 'no=' in href:\n",
    "                    article_no = href.split('no=')[-1]\n",
    "                    try:\n",
    "                        article_no = int(article_no)\n",
    "                        if str(article_no) not in self.processed_articles:\n",
    "                            articles.append(article_no)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取第 {page_no} 頁文章列表失敗: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "        \n",
    "\n",
    "    def fetch_article(self, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"抓取單篇文章\"\"\"\n",
    "        if str(article_no) in self.processed_articles:\n",
    "            return None\n",
    "\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                params = {'no': article_no}\n",
    "                self.wait_between_requests()\n",
    "\n",
    "                self.logger.info(f\"正在抓取文章 {article_no}\")\n",
    "                response = self.session.get(\n",
    "                    self.detail_url,\n",
    "                    params=params,\n",
    "                    timeout=30,\n",
    "                    verify=False\n",
    "                )\n",
    "\n",
    "                self.logger.info(f\"文章 {article_no} 回應狀態碼: {\n",
    "                                 response.status_code}\")\n",
    "\n",
    "                if response.status_code == 404:\n",
    "                    self.logger.info(f\"文章 {article_no} 不存在\")\n",
    "                    return None\n",
    "\n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                article_data = self.parse_article(soup, article_no)\n",
    "\n",
    "                if article_data and self.validate_article(article_data):\n",
    "                    self.logger.info(f\"成功解析文章 {article_no}\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    self.logger.info(f\"文章 {article_no} 不符合條件或解析失敗\")\n",
    "                    return None\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(f\"抓取文章 {article_no} 失敗 (嘗試 {\n",
    "                                  retry + 1}/{self.max_retries}): {str(e)}\")\n",
    "                if hasattr(e, 'response') and hasattr(e.response, 'text'):\n",
    "                    self.logger.error(f\"錯誤回應內容: {e.response.text[:200]}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (retry + 1))\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"處理文章 {article_no} 時發生未預期錯誤: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        return None\n",
    "\n",
    "    def parse_article(self, soup: BeautifulSoup, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"解析文章內容\"\"\"\n",
    "        try:\n",
    "            author = None\n",
    "            article_info = {}\n",
    "\n",
    "            for row in soup.select('.columnsDetail_tableRow'):\n",
    "                th = row.select_one('.columnsDetail_tableth')\n",
    "                td = row.select_one('.columnsDetail_tabletd')\n",
    "                if th and td:\n",
    "                    key = th.text.strip()\n",
    "                    value = td.text.strip()\n",
    "                    article_info[key] = value\n",
    "\n",
    "                    if key == '內文':\n",
    "                        article_info['內文HTML'] = str(td)\n",
    "                    elif key == '作者':\n",
    "                        author = value\n",
    "\n",
    "            # 如果不是目標作者就直接返回\n",
    "            if not author or not any(target in author for target in self.target_authors):\n",
    "                return None\n",
    "\n",
    "            # 處理文章內容和圖片\n",
    "            content = self.process_content(\n",
    "                article_info.get('內文HTML', ''), article_no)\n",
    "\n",
    "            return {\n",
    "                '文章編號': article_no,\n",
    "                '標題': article_info.get('篇名', ''),\n",
    "                '作者': author,\n",
    "                '日期': article_info.get('日期', ''),\n",
    "                '內文': content,\n",
    "                'URL': f\"{self.detail_url}?no={article_no}\",\n",
    "                '爬取時間': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"解析文章 {article_no} 失敗: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def process_content(self, html_content: str, article_no: int) -> str:\n",
    "        \"\"\"處理文章內容，包括圖片下載和格式轉換\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            content_parts = []\n",
    "\n",
    "            # 處理文字段落\n",
    "            for p in soup.find_all(['p', 'div']):\n",
    "                text = p.get_text().strip()\n",
    "                if text:\n",
    "                    content_parts.append(text)\n",
    "\n",
    "            # 處理圖片\n",
    "            for img in soup.find_all('img'):\n",
    "                img_url = img.get('src', '')\n",
    "                if not img_url:\n",
    "                    continue\n",
    "\n",
    "                if not img_url.startswith(('http://', 'https://')):\n",
    "                    img_url = urllib.parse.urljoin(self.base_url, img_url)\n",
    "\n",
    "                img_filename = self.download_image(img_url, article_no)\n",
    "                if img_filename:\n",
    "                    content_parts.append(\n",
    "                        f'\\n![圖片](../images/{img_filename})\\n')\n",
    "\n",
    "            return '\\n\\n'.join(content_parts)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"處理文章 {article_no} 內容時發生錯誤: {str(e)}\")\n",
    "            return html_content\n",
    "\n",
    "    def download_image(self, img_url: str, article_no: int) -> Optional[str]:\n",
    "        \"\"\"下載並保存圖片\"\"\"\n",
    "        try:\n",
    "            # 生成唯一的檔案名\n",
    "            url_hash = hashlib.md5(img_url.encode()).hexdigest()\n",
    "            img_ext = os.path.splitext(urllib.parse.urlparse(img_url).path)[1]\n",
    "            if not img_ext:\n",
    "                img_ext = '.jpg'\n",
    "            filename = f\"article_{article_no}_{url_hash}{img_ext}\"\n",
    "\n",
    "            img_path = self.images_dir / filename\n",
    "\n",
    "            # 如果圖片已存在，直接返回檔名\n",
    "            if img_path.exists():\n",
    "                return filename\n",
    "\n",
    "            # 下載圖片\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                img_url,\n",
    "                timeout=30,\n",
    "                verify=False,\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 檢查是否為有效的圖片\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            if not content_type.startswith('image/'):\n",
    "                raise ValueError(f\"無效的圖片類型: {content_type}\")\n",
    "\n",
    "            # 保存圖片\n",
    "            with open(img_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "            return filename\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"下載圖片失敗 ({img_url}): {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_article(self, article_data: Dict[str, Any]) -> bool:\n",
    "        \"\"\"保存文章到 Markdown 文件和 Excel\"\"\"\n",
    "        try:\n",
    "            article_no = article_data['文章編號']\n",
    "\n",
    "            # 建立 Markdown 內容\n",
    "            md_content = f\"# {article_data['標題']}\\n\\n\"\n",
    "            md_content += f\"作者: {article_data['作者']}\\n\"\n",
    "            md_content += f\"日期: {article_data['日期']}\\n\"\n",
    "            md_content += f\"來源: {article_data['URL']}\\n\\n\"\n",
    "            md_content += f\"*注：本文圖片存放於 ../images/ 目錄下*\\n\\n\"\n",
    "            md_content += \"---\\n\\n\"\n",
    "            md_content += article_data['內文']\n",
    "\n",
    "            # 保存 Markdown 文件\n",
    "            md_file = self.articles_dir / f\"article_{article_no}.md\"\n",
    "            with open(md_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(md_content)\n",
    "\n",
    "            # 更新 Excel 文件\n",
    "            df_new = pd.DataFrame([article_data])\n",
    "            if self.data_file.exists():\n",
    "                df_existing = pd.read_excel(self.data_file)\n",
    "                df = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "            else:\n",
    "                df = df_new\n",
    "\n",
    "            df.to_excel(self.data_file, index=False)\n",
    "\n",
    "            # 更新已處理文章集合\n",
    "            self.processed_articles.add(str(article_no))\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"保存文章 {article_data['文章編號']} 失敗: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def validate_article(self, article_data: Dict) -> bool:\n",
    "        \"\"\"驗證文章資料的有效性\"\"\"\n",
    "        if not article_data:\n",
    "            return False\n",
    "\n",
    "        required_fields = ['文章編號', '標題', '作者', '日期', '內文']\n",
    "        if not all(field in article_data for field in required_fields):\n",
    "            return False\n",
    "\n",
    "        # 檢查文章日期\n",
    "        try:\n",
    "            article_date = datetime.strptime(article_data['日期'], '%Y-%m-%d')\n",
    "            if not (self.start_date <= article_date <= self.end_date):\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "        # 檢查作者\n",
    "        if not any(author in article_data['作者'] for author in self.target_authors):\n",
    "            return False\n",
    "\n",
    "        # 檢查內容長度\n",
    "        if len(article_data['內文'].strip()) < 100:  # 最少100字\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def setup_session(self):\n",
    "        \"\"\"設置請求會話\"\"\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        })\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    def setup_logger(self):\n",
    "        \"\"\"設置日誌記錄\"\"\"\n",
    "        self.logger = logging.getLogger('ArticleScraper')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # 檔案處理器\n",
    "        log_file = self.logs_dir / \\\n",
    "            f'scraper_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "        file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # 控制台處理器\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # 設置格式\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def load_processed_articles(self):\n",
    "        \"\"\"載入已處理的文章清單\"\"\"\n",
    "        try:\n",
    "            if self.data_file.exists():\n",
    "                df = pd.read_excel(self.data_file)\n",
    "                self.processed_articles = set(str(no)\n",
    "                                              for no in df['文章編號'].tolist())\n",
    "            else:\n",
    "                self.processed_articles = set()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"載入已處理文章清單失敗: {str(e)}\")\n",
    "            self.processed_articles = set()\n",
    "\n",
    "    def wait_between_requests(self):\n",
    "        \"\"\"控制請求間隔\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.request_interval:\n",
    "            time.sleep(self.request_interval - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def process_article_batch(self, article_numbers: List[int]):\n",
    "        \"\"\"處理一批文章\"\"\"\n",
    "        for article_no in article_numbers:\n",
    "            try:\n",
    "                article_data = self.fetch_article(article_no)\n",
    "                if article_data and self.validate_article(article_data):\n",
    "                    if self.save_article(article_data):\n",
    "                        self.logger.info(f\"成功保存文章 {article_no}\")\n",
    "                    else:\n",
    "                        self.logger.error(f\"保存文章 {article_no} 失敗\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"處理文章 {article_no} 時發生錯誤: {str(e)}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"主要執行函數\"\"\"\n",
    "        self.logger.info(\"開始執行文章爬蟲\")\n",
    "\n",
    "        try:\n",
    "            if self.scan_mode == \"journal\":\n",
    "                # 從期刊頁面獲取文章\n",
    "                max_page = self.get_max_page_number()\n",
    "                self.logger.info(f\"找到 {max_page} 頁期刊\")\n",
    "\n",
    "                all_articles = []\n",
    "                for page in range(1, max_page + 1):\n",
    "                    articles = self.get_article_urls_from_journal(page)\n",
    "                    all_articles.extend(articles)\n",
    "\n",
    "                self.logger.info(f\"從期刊頁面找到 {len(all_articles)} 篇文章\")\n",
    "\n",
    "            else:  # scan_mode == \"all\"\n",
    "                # 掃描所有可能的文章編號\n",
    "                all_articles = list(range(self.start_no, self.max_no + 1))\n",
    "                self.logger.info(f\"將掃描 {len(all_articles)} 個可能的文章編號\")\n",
    "\n",
    "            # 分批處理文章\n",
    "            batches = [all_articles[i:i + self.batch_size]\n",
    "                       for i in range(0, len(all_articles), self.batch_size)]\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                list(tqdm(\n",
    "                    executor.map(self.process_article_batch, batches),\n",
    "                    total=len(batches),\n",
    "                    desc=\"處理文章批次\"\n",
    "                ))\n",
    "\n",
    "            self.logger.info(\"文章爬蟲執行完成\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"執行過程中發生錯誤: {str(e)}\")\n",
    "        finally:\n",
    "            self.session.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 使用範例\n",
    "    scraper = ArticleScraper(\n",
    "        scan_mode=\"journal\",  # 或 \"all\"\n",
    "        data_file=\"articles.xlsx\"\n",
    "    )\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄已生成至：./README.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "def generate_table_of_contents(excel_path: str, markdown_dir: str, output_path: str):\n",
    "    \"\"\"\n",
    "    生成文章目錄\n",
    "    \n",
    "    Args:\n",
    "        excel_path: Excel檔案路徑，包含文章metadata\n",
    "        markdown_dir: Markdown文件目錄\n",
    "        output_path: 輸出目錄檔案路徑\n",
    "    \"\"\"\n",
    "    # 讀取Excel\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # 轉換日期欄位\n",
    "    df['日期'] = pd.to_datetime(df['日期'])\n",
    "\n",
    "    # 生成目錄內容\n",
    "    toc_content = \"# 文章目錄\\n\\n\"\n",
    "\n",
    "    # 按日期排序（從新到舊）\n",
    "    df = df.sort_values('日期', ascending=False)\n",
    "\n",
    "    # 依年份分組\n",
    "    for year in df['日期'].dt.year.unique():\n",
    "        toc_content += f\"\\n## {year}年\\n\\n\"\n",
    "        year_df = df[df['日期'].dt.year == year]\n",
    "\n",
    "        # 依月份分組\n",
    "        for month in year_df['日期'].dt.month.unique():\n",
    "            toc_content += f\"\\n### {month}月\\n\\n\"\n",
    "            month_df = year_df[year_df['日期'].dt.month == month]\n",
    "\n",
    "            # 生成每篇文章的連結\n",
    "            for _, article in month_df.iterrows():\n",
    "                clean_title = re.sub(r'[<>:\"/\\\\|?*]', '', str(article['標題']))\n",
    "                md_path = Path(markdown_dir) / \\\n",
    "                    f\"{article['文章編號']}_{clean_title}.md\"\n",
    "\n",
    "                if md_path.exists():\n",
    "                    relative_path = f\"./articles/{md_path.name}\"\n",
    "                    date_str = article['日期'].strftime('%Y-%m-%d')\n",
    "                    toc_content += f\"- {date_str} [{article['標題']}]({relative_path}) - {\n",
    "                        article['作者']}\\n\"\n",
    "\n",
    "    # 添加生成時間\n",
    "    toc_content += f\"\\n\\n---\\n最後更新時間：{\n",
    "        datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "    # 寫入檔案\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(toc_content)\n",
    "\n",
    "    print(f\"目錄已生成至：{output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 設定路徑\n",
    "    excel_path = \"articles.xlsx\"\n",
    "    markdown_dir = \"./articles\"\n",
    "    output_path = \"./README.md\"\n",
    "\n",
    "    # 生成目錄\n",
    "    generate_table_of_contents(excel_path, markdown_dir, output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
