{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 14:26:11,989 - INFO - 開始執行爬蟲 (模式: new)\n",
      "處理文章:  95%|█████████▍| 468807/493484 [42:10<04:48, 85.62it/s]   "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "import urllib3\n",
    "\n",
    "\n",
    "class ArticleScraper:\n",
    "    def __init__(self, scan_mode=\"all\", data_file=\"articles.xlsx\"):\n",
    "        \"\"\"初始化爬蟲設定\"\"\"\n",
    "        # 基本設定\n",
    "        self.base_url = \"https://real-estate.get.com.tw/Columns/detail.aspx?no=\"\n",
    "        self.target_authors = [\"曾榮耀\", \"許文昌\", \"蘇偉強\"]\n",
    "        self.scan_mode = scan_mode\n",
    "        self.data_file = Path(data_file)\n",
    "\n",
    "        # 時間範圍設定\n",
    "        self.end_date = datetime.now()\n",
    "        self.start_date = self.end_date - timedelta(days=9*365)\n",
    "\n",
    "        # 效能設定\n",
    "        self.batch_size = 200\n",
    "        self.max_workers = 16\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 5\n",
    "\n",
    "        # 文章編號範圍\n",
    "        self.start_no = 409119\n",
    "        self.max_no = 915000\n",
    "\n",
    "        # 初始化\n",
    "        self.setup_directories()\n",
    "        self.setup_session()\n",
    "        self.setup_logger()\n",
    "        self.processed_articles = set()\n",
    "        self.load_processed_articles()\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"建立必要的目錄結構\"\"\"\n",
    "        self.base_dir = Path(\"real_estate_articles\")\n",
    "        self.articles_dir = self.base_dir / \"articles\"\n",
    "        self.images_dir = self.articles_dir / \"images\"\n",
    "        self.logs_dir = self.base_dir / \"logs\"\n",
    "\n",
    "        for directory in [self.base_dir, self.articles_dir, self.images_dir, self.logs_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 創建預設的失敗圖片\n",
    "        self.failed_image_path = self.images_dir / \"image_download_failed.png\"\n",
    "        if not self.failed_image_path.exists():\n",
    "            try:\n",
    "                from PIL import Image, ImageDraw\n",
    "                img = Image.new('RGB', (400, 100), color='white')\n",
    "                d = ImageDraw.Draw(img)\n",
    "                d.text((10, 40), \"Image Download Failed\", fill='black')\n",
    "                img.save(self.failed_image_path)\n",
    "            except Exception:\n",
    "                self.failed_image_path.touch()\n",
    "\n",
    "    def setup_session(self):\n",
    "        \"\"\"設定請求session\"\"\"\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        })\n",
    "\n",
    "    def setup_logger(self):\n",
    "        \"\"\"設定日誌系統\"\"\"\n",
    "        self.logger = logging.getLogger('ArticleScraper')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        log_file = self.logs_dir / \\\n",
    "            f\"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        handlers = [\n",
    "            logging.FileHandler(log_file, encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s')\n",
    "        for handler in handlers:\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def load_processed_articles(self):\n",
    "        \"\"\"載入已處理的文章\"\"\"\n",
    "        if self.data_file.exists():\n",
    "            df = pd.read_excel(self.data_file)\n",
    "            if '文章編號' in df.columns:\n",
    "                self.processed_articles = set(df['文章編號'].astype(str))\n",
    "\n",
    "    def download_image(self, img_url: str, article_no: int) -> str:\n",
    "        \"\"\"下載圖片並返回本地檔名\"\"\"\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                if not img_url.startswith('http'):\n",
    "                    img_url = urllib.parse.urljoin(self.base_url, img_url)\n",
    "\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
    "                    'Accept-Encoding': 'gzip, deflate',\n",
    "                    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "                    'Referer': self.base_url,\n",
    "                    'Connection': 'keep-alive',\n",
    "                    'Cache-Control': 'no-cache',\n",
    "                    'Pragma': 'no-cache'\n",
    "                }\n",
    "\n",
    "                response = self.session.get(\n",
    "                    img_url,\n",
    "                    stream=True,\n",
    "                    timeout=30,\n",
    "                    headers=headers,\n",
    "                    verify=False\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "\n",
    "                content_type = response.headers.get('content-type', '')\n",
    "                if not content_type.startswith('image/'):\n",
    "                    raise ValueError(f\"非圖片內容類型: {content_type}\")\n",
    "\n",
    "                img_data = response.content\n",
    "                img_hash = hashlib.md5(img_data).hexdigest()\n",
    "                img_ext = os.path.splitext(urllib.parse.urlparse(img_url).path)[\n",
    "                    1] or '.jpg'\n",
    "                local_filename = f\"{article_no}_{img_hash}{img_ext}\"\n",
    "                local_path = self.images_dir / local_filename\n",
    "\n",
    "                if not local_path.exists():\n",
    "                    with open(local_path, 'wb') as f:\n",
    "                        f.write(img_data)\n",
    "\n",
    "                return local_filename\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"下載圖片失敗 (嘗試 {retry + 1}/{self.max_retries}) {img_url}: {str(e)}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (retry + 1))\n",
    "                    continue\n",
    "\n",
    "        return \"image_download_failed.png\"\n",
    "\n",
    "    def fetch_article(self, article_no):\n",
    "        \"\"\"抓取單篇文章\"\"\"\n",
    "        if str(article_no) in self.processed_articles:\n",
    "            return None\n",
    "\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                url = f\"{self.base_url}{article_no}\"\n",
    "                response = self.session.get(url, timeout=30, verify=False)\n",
    "\n",
    "                if response.status_code == 404:\n",
    "                    return None\n",
    "\n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                article_data = self.parse_article(soup, article_no, url)\n",
    "\n",
    "                if article_data and self.validate_article(article_data):\n",
    "                    return article_data\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(f\"抓取文章 {article_no} 失敗 (嘗試 {\n",
    "                                  retry + 1}/{self.max_retries}): {str(e)}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (retry + 1))\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"處理文章 {article_no} 時發生未預期錯誤: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        return None\n",
    "\n",
    "    def parse_article(self, soup, article_no, url):\n",
    "        \"\"\"解析文章內容\"\"\"\n",
    "        try:\n",
    "            article_info = {}\n",
    "            for row in soup.select('.columnsDetail_tableRow'):\n",
    "                th = row.select_one('.columnsDetail_tableth')\n",
    "                td = row.select_one('.columnsDetail_tabletd')\n",
    "                if th and td:\n",
    "                    key = th.text.strip()\n",
    "                    value = td.text.strip()\n",
    "                    article_info[key] = value\n",
    "\n",
    "                    if key == '內文':\n",
    "                        article_info['內文HTML'] = td\n",
    "\n",
    "            if not article_info:\n",
    "                return None\n",
    "\n",
    "            if not any(author in article_info.get('作者', '') for author in self.target_authors):\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                article_date = datetime.strptime(\n",
    "                    article_info.get('日期', ''), '%Y/%m/%d')\n",
    "                if not self.start_date <= article_date <= self.end_date:\n",
    "                    return None\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "            return {\n",
    "                '文章編號': article_no,\n",
    "                '標題': article_info.get('篇名', ''),\n",
    "                '作者': article_info.get('作者', ''),\n",
    "                '日期': article_info.get('日期', ''),\n",
    "                '內文': self.process_content(article_info.get('內文HTML'), article_no),\n",
    "                'URL': url,\n",
    "                '爬取時間': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"解析文章 {article_no} 失敗: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_content(self, content_html, article_no):\n",
    "        \"\"\"處理文章內容\"\"\"\n",
    "        if not content_html:\n",
    "            return \"\"\n",
    "\n",
    "        content = []\n",
    "        try:\n",
    "            for element in content_html.descendants:\n",
    "                if isinstance(element, str):\n",
    "                    text = element.strip()\n",
    "                    if text:\n",
    "                        content.append(text)\n",
    "                elif element.name == 'img':\n",
    "                    try:\n",
    "                        img_src = element.get('src')\n",
    "                        if img_src:\n",
    "                            local_img = self.download_image(\n",
    "                                img_src, article_no)\n",
    "                            if local_img:\n",
    "                                content.append(\n",
    "                                    f\"\\n![圖片](./images/{local_img})\\n\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"處理圖片元素失敗: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "            return '\\n'.join(filter(None, content))\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"處理文章 {article_no} 內容失敗: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def validate_article(self, article_data):\n",
    "        \"\"\"驗證文章資料完整性\"\"\"\n",
    "        required_fields = ['標題', '作者', '日期', '內文']\n",
    "        return all(field in article_data and article_data[field] for field in required_fields)\n",
    "\n",
    "    def save_article(self, article_data):\n",
    "        \"\"\"儲存文章\"\"\"\n",
    "        try:\n",
    "            # 更新 Excel 資料\n",
    "            new_df = pd.DataFrame([article_data])\n",
    "            if self.data_file.exists():\n",
    "                df = pd.read_excel(self.data_file)\n",
    "                df = pd.concat([df, new_df]).drop_duplicates(subset=['文章編號'])\n",
    "            else:\n",
    "                df = new_df\n",
    "            df.to_excel(self.data_file, index=False)\n",
    "\n",
    "            # 建立 Markdown 文件\n",
    "            article_no = article_data['文章編號']\n",
    "            title = re.sub(r'[<>:\"/\\\\|?*]', '', article_data['標題'])[:100]\n",
    "\n",
    "            markdown_content = f\"\"\"# {article_data['標題']}\n",
    "\n",
    "## 文章資訊\n",
    "- 文章編號：{article_no}\n",
    "- 作者：{article_data['作者']}\n",
    "- 發布日期：{article_data['日期']}\n",
    "- 爬取時間：{article_data['爬取時間']}\n",
    "- 原文連結：[閱讀原文]({article_data['URL']})\n",
    "\n",
    "## 內文\n",
    "{article_data['內文']}\n",
    "\n",
    "---\n",
    "*注：本文圖片存放於 ./images/ 目錄下*\n",
    "\"\"\"\n",
    "\n",
    "            # 儲存 Markdown 文件\n",
    "            file_path = self.articles_dir / f\"{article_no}_{title}.md\"\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "\n",
    "            # 更新已處理集合\n",
    "            self.processed_articles.add(str(article_no))\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"儲存文章失敗: {str(e)}\")\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"建立索引頁面\"\"\"\n",
    "        try:\n",
    "            # 讀取文章資料\n",
    "            if not self.data_file.exists():\n",
    "                self.logger.error(\"找不到文章資料檔案\")\n",
    "                return\n",
    "\n",
    "            df = pd.read_excel(self.data_file)\n",
    "\n",
    "            # 將日期轉換為datetime格式並排序\n",
    "            df['日期'] = pd.to_datetime(df['日期'])\n",
    "            df = df.sort_values('日期', ascending=False)\n",
    "\n",
    "            # 依年份分組\n",
    "            years = df['日期'].dt.year.unique()\n",
    "\n",
    "            # 建立索引內容\n",
    "            content = [\"# 地政專欄文章索引\\n\"]\n",
    "\n",
    "            # 添加統計資訊\n",
    "            content.append(\"## 文章統計\\n\")\n",
    "            content.append(f\"- 總文章數：{len(df)}篇\")\n",
    "            content.append(f\"- 收錄年份：{min(years)}年 - {max(years)}年\")\n",
    "            content.append(\n",
    "                f\"- 最後更新：{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "            # 作者統計\n",
    "            author_stats = df['作者'].value_counts()\n",
    "            content.append(\"### 作者文章統計\")\n",
    "            for author, count in author_stats.items():\n",
    "                content.append(f\"- {author}: {count}篇\")\n",
    "            content.append(\"\")\n",
    "\n",
    "            content.append(\"## 目錄\\n\")\n",
    "\n",
    "            # 建立年份快速連結\n",
    "            content.append(\"### 年份快速導覽\")\n",
    "            for year in sorted(years, reverse=True):\n",
    "                content.append(f\"- [{year}年](#year-{year})\")\n",
    "            content.append(\"\\n---\\n\")\n",
    "\n",
    "            # 依年份建立文章列表\n",
    "            for year in sorted(years, reverse=True):\n",
    "                content.append(f\"## {year}年 {{#year-{year}}}\\n\")\n",
    "                year_df = df[df['日期'].dt.year == year]\n",
    "\n",
    "                # 依月份分組\n",
    "                for month in sorted(year_df['日期'].dt.month.unique(), reverse=True):\n",
    "                    content.append(f\"### {month}月\\n\")\n",
    "                    month_df = year_df[year_df['日期'].dt.month == month]\n",
    "\n",
    "                    # 列出當月文章\n",
    "                    for _, article in month_df.iterrows():\n",
    "                        title = re.sub(r'[<>:\"/\\\\|?*]', '',\n",
    "                                       article['標題'])[:100]\n",
    "                        article_link = f\"{article['文章編號']}_{title}.md\"\n",
    "                        date_str = article['日期'].strftime('%Y-%m-%d')\n",
    "                        content.append(\n",
    "                            f\"- {date_str} [{article['標題']}](./{article_link}) - {article['作者']}\")\n",
    "                    content.append(\"\")\n",
    "\n",
    "            # 寫入索引文件\n",
    "            index_path = self.articles_dir / \"index.md\"\n",
    "            with open(index_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(content))\n",
    "\n",
    "            self.logger.info(\"索引頁面建立完成\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"建立索引頁面時發生錯誤: {str(e)}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"執行爬蟲\"\"\"\n",
    "        self.logger.info(f\"開始執行爬蟲 (模式: {self.scan_mode})\")\n",
    "\n",
    "        try:\n",
    "            latest_no = int(max(self.processed_articles)\n",
    "                            ) if self.processed_articles else self.start_no\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                futures = []\n",
    "\n",
    "                if self.scan_mode in [\"all\", \"new\"]:\n",
    "                    # 掃描新文章\n",
    "                    for article_no in range(latest_no + 1, self.max_no + 1, self.batch_size):\n",
    "                        batch = range(article_no, min(\n",
    "                            article_no + self.batch_size, self.max_no + 1))\n",
    "                        futures.extend(\n",
    "                            [executor.submit(self.fetch_article, no) for no in batch])\n",
    "\n",
    "                if self.scan_mode in [\"all\", \"old\"]:\n",
    "                    # 掃描舊文章\n",
    "                    for article_no in range(latest_no - 1, self.start_no - 1, -self.batch_size):\n",
    "                        batch = range(article_no, max(\n",
    "                            article_no - self.batch_size, self.start_no - 1), -1)\n",
    "                        futures.extend(\n",
    "                            [executor.submit(self.fetch_article, no) for no in batch])\n",
    "\n",
    "                # 處理結果\n",
    "                with tqdm(total=len(futures), desc=\"處理文章\") as pbar:\n",
    "                    for future in futures:\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result:\n",
    "                                self.save_article(result)\n",
    "                            pbar.update(1)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"處理文章結果失敗: {str(e)}\")\n",
    "                            pbar.update(1)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.info(\"程式被使用者中斷\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"執行過程中發生錯誤: {str(e)}\")\n",
    "        finally:\n",
    "            self.create_index()  # 建立索引頁面\n",
    "            self.logger.info(\"程式結束執行\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 建立爬蟲實例並執行\n",
    "    scraper = ArticleScraper(scan_mode=\"new\")\n",
    "    scraper.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
