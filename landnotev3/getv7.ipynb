{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可以更新最近上傳的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 10:08:47,550 - INFO - 初始化更新模式: recent\n",
      "2025-03-08 10:08:47,558 - INFO - 更新時間範圍: 2025-02-06 到 2025-03-08\n",
      "2025-03-08 10:08:47,559 - INFO - 開始檢查文章更新 (檢查範圍: 2025-02-06 到 2025-03-08)\n",
      "2025-03-08 10:08:48,390 - INFO - 檢測到期刊總頁數: 1\n",
      "2025-03-08 10:08:48,392 - INFO - 正在檢查第 1 頁的文章列表\n",
      "2025-03-08 10:08:49,129 - INFO - 第 1 頁找到 4 篇新文章\n",
      "2025-03-08 10:08:49,129 - INFO - 找到 3 篇待檢查文章\n",
      "更新文章:   0%|          | 0/3 [00:00<?, ?it/s]2025-03-08 10:08:49,902 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913445\n",
      "2025-03-08 10:08:49,902 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913460\n",
      "2025-03-08 10:08:49,903 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913517\n",
      "2025-03-08 10:08:49,933 - INFO - 文章 913445 請求狀態碼: 200\n",
      "2025-03-08 10:08:49,936 - INFO - 文章 913445 響應內容長度: 30337\n",
      "2025-03-08 10:08:50,021 - INFO - 已保存文章 913445 的原始響應到: real_estate_articles\\logs\\article_913445_response.html\n",
      "2025-03-08 10:08:50,067 - INFO - 文章 913445 找到 9 個資料列\n",
      "2025-03-08 10:08:50,071 - INFO - 文章 913445 成功解析 4 個欄位\n",
      "2025-03-08 10:08:50,075 - INFO - 文章 913445 解析成功\n",
      "2025-03-08 10:08:50,812 - INFO - 文章 913517 請求狀態碼: 200\n",
      "2025-03-08 10:08:50,879 - INFO - 文章 913460 請求狀態碼: 200\n",
      "2025-03-08 10:08:50,879 - INFO - 文章 913517 響應內容長度: 30809\n",
      "2025-03-08 10:08:50,879 - INFO - 文章 913460 響應內容長度: 30672\n",
      "2025-03-08 10:08:50,879 - INFO - 已保存文章 913517 的原始響應到: real_estate_articles\\logs\\article_913517_response.html\n",
      "2025-03-08 10:08:50,896 - INFO - 已保存文章 913460 的原始響應到: real_estate_articles\\logs\\article_913460_response.html\n",
      "2025-03-08 10:08:50,934 - INFO - 文章 913517 找到 8 個資料列\n",
      "2025-03-08 10:08:50,969 - INFO - 文章 913460 找到 9 個資料列\n",
      "2025-03-08 10:08:50,969 - INFO - 文章 913517 成功解析 4 個欄位\n",
      "2025-03-08 10:08:50,969 - INFO - 文章 913460 成功解析 4 個欄位\n",
      "2025-03-08 10:08:50,986 - INFO - 文章 913460 解析成功\n",
      "2025-03-08 10:08:51,020 - INFO - 文章 913517 解析成功\n",
      "更新文章: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n",
      "2025-03-08 10:08:53,382 - INFO - 更新完成：成功 3 篇，失敗 0 篇\n",
      "2025-03-08 10:08:53,920 - INFO - 已成功更新文章目錄，新增 3 篇文章\n",
      "2025-03-08 10:08:53,921 - INFO - 更新程序結束\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "import urllib3\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "\n",
    "class ArticleScraper:\n",
    "    def __init__(self, scan_mode=\"recent\", data_file=\"articles.xlsx\"):\n",
    "        \"\"\"初始化爬蟲設定\"\"\"\n",
    "        # 常量設定\n",
    "        self.SETTINGS = {\n",
    "            'BASE_URL': \"https://real-estate.get.com.tw/Columns/\",\n",
    "            'TARGET_AUTHORS': [\"曾榮耀\", \"許文昌\", \"蘇偉強\"],\n",
    "            'JOURNAL_PARAMS': {\n",
    "                \"no\": \"1282\",\n",
    "                \"pno\": \"51121\"\n",
    "            },\n",
    "            'PERFORMANCE': {\n",
    "                'BATCH_SIZE': 50,\n",
    "                'MAX_WORKERS': 4,\n",
    "                'MAX_RETRIES': 5,\n",
    "                'RETRY_DELAY': 3,\n",
    "                'REQUEST_INTERVAL': 1.5\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 基本設定\n",
    "        self.base_url = self.SETTINGS['BASE_URL']\n",
    "        self.detail_url = f\"{self.base_url}detail.aspx\"\n",
    "        self.journal_url = f\"{self.base_url}journal.aspx\"\n",
    "        self.target_authors = self.SETTINGS['TARGET_AUTHORS']\n",
    "        self.scan_mode = scan_mode\n",
    "        self.data_file = Path(data_file)\n",
    "\n",
    "        # 期刊參數設定\n",
    "        self.journal_params = self.SETTINGS['JOURNAL_PARAMS']\n",
    "\n",
    "        # 時間範圍設定 - 修改為30天\n",
    "        self.end_date = datetime.now()\n",
    "        self.start_date = self.end_date - timedelta(days=30)\n",
    "\n",
    "        # 效能設定\n",
    "        self.batch_size = self.SETTINGS['PERFORMANCE']['BATCH_SIZE']\n",
    "        self.max_workers = self.SETTINGS['PERFORMANCE']['MAX_WORKERS']\n",
    "        self.max_retries = self.SETTINGS['PERFORMANCE']['MAX_RETRIES']\n",
    "        self.retry_delay = self.SETTINGS['PERFORMANCE']['RETRY_DELAY']\n",
    "        self.request_interval = self.SETTINGS['PERFORMANCE']['REQUEST_INTERVAL']\n",
    "\n",
    "        # 移除文章範圍設定，因為只需要更新最新文章\n",
    "        \n",
    "        # 初始化其他組件\n",
    "        self.setup_directories()\n",
    "        self.setup_session()\n",
    "        self.setup_logger()\n",
    "        self.processed_articles = set()\n",
    "        self.load_processed_articles()\n",
    "        self.last_request_time = 0\n",
    "\n",
    "        # 記錄掃描設定到日誌\n",
    "        self.logger.info(f\"初始化更新模式: {scan_mode}\")\n",
    "        self.logger.info(f\"更新時間範圍: {self.start_date.date()} 到 {self.end_date.date()}\")\n",
    "\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"建立必要的目錄結構\"\"\"\n",
    "        self.base_dir = Path(\"real_estate_articles\")\n",
    "        self.articles_dir = self.base_dir / \"articles\"\n",
    "        self.images_dir = self.articles_dir / \"images\"\n",
    "        self.logs_dir = self.base_dir / \"logs\"\n",
    "\n",
    "        for directory in [self.base_dir, self.articles_dir, self.images_dir, self.logs_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 創建預設的失敗圖片\n",
    "        self.failed_image_path = self.images_dir / \"image_download_failed.png\"\n",
    "        if not self.failed_image_path.exists():\n",
    "            try:\n",
    "                from PIL import Image, ImageDraw\n",
    "                img = Image.new('RGB', (400, 100), color='white')\n",
    "                d = ImageDraw.Draw(img)\n",
    "                d.text((10, 40), \"Image Download Failed\", fill='black')\n",
    "                img.save(self.failed_image_path)\n",
    "            except Exception:\n",
    "                self.failed_image_path.touch()\n",
    "\n",
    "   \n",
    "\n",
    "    def setup_session(self):\n",
    "        \"\"\"設定請求session\"\"\"\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    def setup_logger(self):\n",
    "        \"\"\"設定日誌系統\"\"\"\n",
    "        self.logger = logging.getLogger('ArticleScraper')\n",
    "        # 清除現有的 handlers\n",
    "        self.logger.handlers = []\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        log_file = self.logs_dir / f\"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        handlers = [\n",
    "            logging.FileHandler(log_file, encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        for handler in handlers:\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "\n",
    "    def load_processed_articles(self):\n",
    "        \"\"\"載入已處理的文章\"\"\"\n",
    "        if self.data_file.exists():\n",
    "            df = pd.read_excel(self.data_file)\n",
    "            if '文章編號' in df.columns:\n",
    "                self.processed_articles = set(df['文章編號'].astype(str))\n",
    "\n",
    "    def wait_between_requests(self):\n",
    "        \"\"\"控制請求間隔\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_request_time\n",
    "        if elapsed < self.request_interval:\n",
    "            time.sleep(self.request_interval - elapsed)\n",
    "        self.last_request_time = current_time\n",
    "\n",
    "    def get_max_page_number(self) -> int:\n",
    "        \"\"\"獲取期刊最大頁數\"\"\"\n",
    "        try:\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=self.journal_params,\n",
    "                timeout=30,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 找到分頁元素\n",
    "            pagination = soup.select('.pagination a')\n",
    "            if pagination:\n",
    "                page_numbers = []\n",
    "                for a in pagination:\n",
    "                    try:\n",
    "                        page_numbers.append(int(a.text.strip()))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                return max(page_numbers) if page_numbers else 1\n",
    "            return 1\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取最大頁數失敗: {str(e)}\")\n",
    "            return 30  # 預設較大的頁數以確保不遺漏\n",
    "\n",
    "    def get_article_urls_from_journal(self, page_no: int) -> List[int]:\n",
    "        \"\"\"從期刊頁面獲取文章編號列表\"\"\"\n",
    "        try:\n",
    "            params = self.journal_params.copy()\n",
    "            params['page_no'] = page_no\n",
    "\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=params,\n",
    "                timeout=30,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 尋找文章列表\n",
    "            articles = []\n",
    "            for link in soup.select('a[href*=\"detail.aspx?no=\"]'):\n",
    "                try:\n",
    "                    href = link.get('href', '')\n",
    "                    if match := re.search(r'no=(\\d+)', href):\n",
    "                        article_no = int(match.group(1))\n",
    "                        # 檢查發布日期（如果有的話）\n",
    "                        date_element = link.find_next('td', class_='date')\n",
    "                        if date_element:\n",
    "                            date_str = date_element.text.strip()\n",
    "                            try:\n",
    "                                pub_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                                if pub_date < self.start_date:\n",
    "                                    continue\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "                        \n",
    "                        if str(article_no) not in self.processed_articles:\n",
    "                            articles.append(article_no)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            if articles:\n",
    "                self.logger.info(f\"第 {page_no} 頁找到 {len(articles)} 篇新文章\")\n",
    "            \n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取第 {page_no} 頁文章列表失敗: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "    def fetch_article(self, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"抓取單篇文章\"\"\"\n",
    "        if str(article_no) in self.processed_articles:\n",
    "            self.logger.debug(f\"文章 {article_no} 已處理過，跳過\")\n",
    "            return None\n",
    "\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                self.wait_between_requests()\n",
    "                \n",
    "                url = f\"{self.detail_url}?no={article_no}\"\n",
    "                self.logger.info(f\"開始請求文章 URL: {url}\")  # 改為 INFO 級別\n",
    "                \n",
    "                # 使用 session 而不是直接使用 requests\n",
    "                response = self.session.get(\n",
    "                    url,\n",
    "                    timeout=30,\n",
    "                    verify=False\n",
    "                )\n",
    "                \n",
    "                # 記錄響應狀態\n",
    "                self.logger.info(f\"文章 {article_no} 請求狀態碼: {response.status_code}\")\n",
    "                \n",
    "                if response.status_code == 404:\n",
    "                    self.logger.error(f\"文章 {article_no} 不存在 (404)\")\n",
    "                    return None\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "                \n",
    "                # 記錄響應內容長度\n",
    "                content_length = len(response.text)\n",
    "                self.logger.info(f\"文章 {article_no} 響應內容長度: {content_length}\")\n",
    "                \n",
    "                if content_length < 100:  # 假設正常文章至少有100字符\n",
    "                    self.logger.error(f\"文章 {article_no} 響應內容過短，可能是無效響應\")\n",
    "                    return None\n",
    "                \n",
    "                # 保存原始響應以供調試\n",
    "                debug_file = self.logs_dir / f\"article_{article_no}_response.html\"\n",
    "                with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                self.logger.info(f\"已保存文章 {article_no} 的原始響應到: {debug_file}\")\n",
    "                \n",
    "                # 解析HTML\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 檢查頁面是否包含預期的元素\n",
    "                if not soup.select('.columnsDetail_tableRow'):\n",
    "                    self.logger.error(f\"文章 {article_no} 頁面結構不符合預期\")\n",
    "                    return None\n",
    "                \n",
    "                # 使用 parse_article 方法解析內容\n",
    "                article_data = self.parse_article(soup, article_no)\n",
    "                \n",
    "                if article_data and self.validate_article(article_data):\n",
    "                    self.logger.info(f\"文章 {article_no} 解析成功\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    self.logger.error(f\"文章 {article_no} 解析失敗或不符合條件\")\n",
    "                    return None\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(f\"抓取文章 {article_no} 失敗 (嘗試 {retry + 1}/{self.max_retries}): {str(e)}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    wait_time = self.retry_delay * (retry + 1)\n",
    "                    self.logger.info(f\"等待 {wait_time} 秒後重試...\")\n",
    "                    time.sleep(wait_time)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"處理文章 {article_no} 時發生未預期錯誤: {str(e)}\")\n",
    "                import traceback\n",
    "                self.logger.error(f\"錯誤堆疊: {traceback.format_exc()}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    wait_time = self.retry_delay * (retry + 1)\n",
    "                    self.logger.info(f\"等待 {wait_time} 秒後重試...\")\n",
    "                    time.sleep(wait_time)\n",
    "                continue\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    def check_specific_article(self, article_no: int):\n",
    "        \"\"\"檢查特定文章\"\"\"\n",
    "        self.logger.info(f\"開始檢查特定文章: {article_no}\")\n",
    "        try:\n",
    "            result = self.fetch_article(article_no)\n",
    "            if result:\n",
    "                self.logger.info(f\"文章 {article_no} 抓取成功，開始保存\")\n",
    "                self.save_article(result)\n",
    "                self.create_index()\n",
    "                self.logger.info(f\"成功抓取並保存文章 {article_no}\")\n",
    "            else:\n",
    "                self.logger.error(f\"無法抓取文章 {article_no}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"檢查文章 {article_no} 時發生錯誤: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def parse_article(self, soup: BeautifulSoup, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"解析文章內容\"\"\"\n",
    "        try:\n",
    "            article_info = {}\n",
    "            \n",
    "            # 記錄找到的欄位數量\n",
    "            found_fields = 0\n",
    "            \n",
    "            # 解析文章基本信息\n",
    "            rows = soup.select('.columnsDetail_tableRow')\n",
    "            self.logger.info(f\"文章 {article_no} 找到 {len(rows)} 個資料列\")\n",
    "            \n",
    "            for row in rows:\n",
    "                th = row.select_one('.columnsDetail_tableth')\n",
    "                td = row.select_one('.columnsDetail_tabletd')\n",
    "                \n",
    "                if th and td:\n",
    "                    key = th.text.strip()\n",
    "                    value = td.text.strip()\n",
    "                    \n",
    "                    self.logger.debug(f\"文章 {article_no} 欄位: {key} = {value[:50]}...\")\n",
    "                    \n",
    "                    if key == '篇名':\n",
    "                        article_info['標題'] = value\n",
    "                        found_fields += 1\n",
    "                    elif key == '作者':\n",
    "                        article_info['作者'] = value\n",
    "                        found_fields += 1\n",
    "                    elif key == '日期':\n",
    "                        article_info['日期'] = value\n",
    "                        found_fields += 1\n",
    "                    elif key == '內文':\n",
    "                        article_info['內文HTML'] = str(td)\n",
    "                        found_fields += 1\n",
    "\n",
    "            self.logger.info(f\"文章 {article_no} 成功解析 {found_fields} 個欄位\")\n",
    "\n",
    "            # 驗證必要欄位\n",
    "            missing_fields = []\n",
    "            for field in ['標題', '作者', '日期']:\n",
    "                if field not in article_info:\n",
    "                    missing_fields.append(field)\n",
    "            \n",
    "            if missing_fields:\n",
    "                self.logger.error(f\"文章 {article_no} 缺少必要欄位: {', '.join(missing_fields)}\")\n",
    "                return None\n",
    "\n",
    "            # 驗證作者\n",
    "            if not any(target in article_info['作者'] for target in self.target_authors):\n",
    "                self.logger.info(f\"文章 {article_no} 作者不符合目標: {article_info['作者']}\")\n",
    "                return None\n",
    "\n",
    "            # 處理文章內容\n",
    "            content = self.process_content(article_info.get('內文HTML', ''), article_no)\n",
    "            \n",
    "            if not content:\n",
    "                self.logger.error(f\"文章 {article_no} 內文處理後為空\")\n",
    "                return None\n",
    "            \n",
    "            # 構建完整的文章數據\n",
    "            article_data = {\n",
    "                '文章編號': article_no,\n",
    "                '標題': article_info['標題'],\n",
    "                '作者': article_info['作者'],\n",
    "                '日期': article_info['日期'],\n",
    "                '內文': content,\n",
    "                'URL': f\"{self.detail_url}?no={article_no}\",\n",
    "                '爬取時間': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "\n",
    "            return article_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"解析文章 {article_no} 失敗: {str(e)}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"錯誤堆疊: {traceback.format_exc()}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def download_image(self, img_url: str, article_no: int) -> Optional[str]:\n",
    "        \"\"\"下載圖片並返回本地檔名\"\"\"\n",
    "        try:\n",
    "            if not img_url.startswith(('http://', 'https://')):\n",
    "                img_url = urllib.parse.urljoin(self.base_url, img_url)\n",
    "\n",
    "            # 生成唯一的檔名\n",
    "            url_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]\n",
    "            file_ext = os.path.splitext(img_url)[1] or '.jpg'\n",
    "            local_filename = f\"{article_no}_{url_hash}{file_ext}\"\n",
    "            local_path = self.images_dir / local_filename\n",
    "\n",
    "            # 如果圖片已存在就直接返回\n",
    "            if local_path.exists():\n",
    "                return local_filename\n",
    "\n",
    "            # 下載圖片\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(img_url, timeout=30, verify=False)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 保存圖片\n",
    "            with open(local_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            return local_filename\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"下載圖片失敗 ({img_url}): {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_content(self, html_content: str, article_no: int) -> str:\n",
    "        \"\"\"處理文章內容，包括下載圖片和清理HTML\"\"\"\n",
    "        if not html_content:\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        image_references = []  # 用於存儲圖片引用\n",
    "\n",
    "        # 處理圖片\n",
    "        for index, img in enumerate(soup.find_all('img'), 1):\n",
    "            img_url = img.get('src', '')\n",
    "            if img_url:\n",
    "                local_filename = self.download_image(img_url, article_no)\n",
    "                if local_filename:\n",
    "                    # 保存圖片引用\n",
    "                    image_references.append(\n",
    "                        f\"\\n![圖片{index}](./images/{local_filename})\\n\")\n",
    "                    # 在原文中插入圖片標記\n",
    "                    img.replace_with(f\"[圖片{index}]\")\n",
    "                else:\n",
    "                    img.replace_with(\"[圖片下載失敗]\")\n",
    "\n",
    "        # 清理HTML並格式化\n",
    "        content = self._format_content(soup)\n",
    "\n",
    "        # 在文章末尾添加所有圖片\n",
    "        if image_references:\n",
    "            content += \"\\n\\n## 文章圖片\\n\"\n",
    "            content += \"\".join(image_references)\n",
    "\n",
    "        return content\n",
    "\n",
    "    def _format_content(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"格式化文章內容，處理換行和縮排\"\"\"\n",
    "        # 允許的HTML標籤\n",
    "        allowed_tags = {'p', 'br', 'h1', 'h2', 'h3',\n",
    "                        'h4', 'h5', 'h6', 'ul', 'ol', 'li'}\n",
    "\n",
    "        # 移除不允許的標籤但保留其文本\n",
    "        for tag in soup.find_all():\n",
    "            if tag.name not in allowed_tags:\n",
    "                tag.unwrap()\n",
    "\n",
    "        # 確保段落之間有適當的換行\n",
    "        for p in soup.find_all('p'):\n",
    "            text = p.get_text().strip()\n",
    "            if text:  # 只處理非空段落\n",
    "                p.string = ' '.join(text.split())\n",
    "                p.append('\\n\\n')\n",
    "\n",
    "        # 處理標題\n",
    "        for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            level = int(h.name[1])  # 獲取標題級別\n",
    "            prefix = '#' * level + ' '  # 根據級別添加 # 符號\n",
    "            h.string = f'\\n{prefix}{h.get_text().strip()}\\n'\n",
    "\n",
    "        # 處理列表\n",
    "        for li in soup.find_all('li'):\n",
    "            indent = '  '\n",
    "            if li.parent.name == 'ol':\n",
    "                # 為有序列表添加數字\n",
    "                index = len(li.find_previous_siblings('li')) + 1\n",
    "                li.insert(0, f'{indent}{index}. ')\n",
    "            else:\n",
    "                # 為無序列表添加圓點\n",
    "                li.insert(0, f'{indent}• ')\n",
    "            li.append('\\n')\n",
    "\n",
    "        # 獲取處理後的文本\n",
    "        content = soup.get_text()\n",
    "\n",
    "        # 清理最終文本\n",
    "        content = re.sub(r'\\n{3,}', '\\n\\n', content)  # 移除過多的空行\n",
    "        content = re.sub(r'[ \\t]+', ' ', content)     # 標準化空格\n",
    "        content = re.sub(r' *\\n *', '\\n', content)    # 清理行首尾空格\n",
    "\n",
    "        # 分段處理並保持段落間的空行\n",
    "        paragraphs = [p.strip() for p in content.split('\\n') if p.strip()]\n",
    "        formatted_content = '\\n\\n'.join(paragraphs)\n",
    "\n",
    "        return formatted_content.strip()\n",
    "\n",
    "    def validate_article(self, article_data: Dict) -> bool:\n",
    "        \"\"\"驗證文章資料完整性\"\"\"\n",
    "        required_fields = ['標題', '作者', '日期', '內文']\n",
    "        return all(field in article_data and article_data[field] for field in required_fields)\n",
    "\n",
    "    def save_article(self, article_data: Dict) -> None:\n",
    "        \"\"\"儲存文章\"\"\"\n",
    "        try:\n",
    "            # 更新 Excel 資料\n",
    "            new_df = pd.DataFrame([article_data])\n",
    "            if self.data_file.exists():\n",
    "                df = pd.read_excel(self.data_file)\n",
    "                df = pd.concat([df, new_df]).drop_duplicates(subset=['文章編號'])\n",
    "            else:\n",
    "                df = new_df\n",
    "            df.to_excel(self.data_file, index=False)\n",
    "\n",
    "            # 建立 Markdown 文件\n",
    "            article_no = article_data['文章編號']\n",
    "            title = re.sub(r'[<>:\"/\\\\|?*]', '', article_data['標題'])[:100]\n",
    "\n",
    "            markdown_content = f\"\"\"# {article_data['標題']}\n",
    "\n",
    "## 文章資訊\n",
    "- 文章編號：{article_no}\n",
    "- 作者：{article_data['作者']}\n",
    "- 發布日期：{article_data['日期']}\n",
    "- 爬取時間：{article_data['爬取時間']}\n",
    "- 原文連結：[閱讀原文]({article_data['URL']})\n",
    "\n",
    "## 內文\n",
    "{article_data['內文']}\n",
    "\n",
    "---\n",
    "*注：本文圖片存放於 ./images/ 目錄下*\n",
    "\"\"\"\n",
    "\n",
    "            # 儲存 Markdown 文件\n",
    "            file_path = self.articles_dir / f\"{article_no}_{title}.md\"\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "\n",
    "            # 更新已處理集合\n",
    "            self.processed_articles.add(str(article_no))\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"儲存文章失敗: {str(e)}\")\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"創建文章索引，整合原有目錄內容\"\"\"\n",
    "        try:\n",
    "            # 讀取 Excel 檔案獲取所有文章資訊\n",
    "            if not self.data_file.exists():\n",
    "                return\n",
    "\n",
    "            df = pd.read_excel(self.data_file)\n",
    "\n",
    "            # 讀取現有的目錄文件（如果存在）\n",
    "            index_path = self.base_dir / \"README.md\"\n",
    "            existing_content = []\n",
    "            existing_articles = set()\n",
    "\n",
    "            if index_path.exists():\n",
    "                with open(index_path, 'r', encoding='utf-8') as f:\n",
    "                    existing_content = f.read().splitlines()\n",
    "                    # 提取現有目錄中的文章編號\n",
    "                    for line in existing_content:\n",
    "                        if match := re.search(r'/(\\d+)_[^/]+\\.md', line):\n",
    "                            existing_articles.add(match.group(1))\n",
    "\n",
    "            # 只處理新文章\n",
    "            df['文章編號'] = df['文章編號'].astype(str)\n",
    "            new_articles = df[~df['文章編號'].isin(existing_articles)]\n",
    "\n",
    "            if new_articles.empty and existing_content:\n",
    "                self.logger.info(\"沒有新文章需要添加到目錄\")\n",
    "                return\n",
    "\n",
    "            # 如果有現有內容，保留開頭的通用部分（如標題）\n",
    "            index_content = []\n",
    "            for line in existing_content:\n",
    "                if line.startswith('## '):\n",
    "                    break\n",
    "                index_content.append(line)\n",
    "\n",
    "            if not index_content:  # 如果是全新的目錄\n",
    "                index_content = [\"# 文章目錄\", \"\"]\n",
    "\n",
    "            # 合併現有文章和新文章\n",
    "            all_articles = pd.concat([\n",
    "                df[df['文章編號'].isin(existing_articles)],\n",
    "                new_articles\n",
    "            ]).drop_duplicates(subset=['文章編號'])\n",
    "\n",
    "            # 按作者和日期排序\n",
    "            all_articles['日期'] = pd.to_datetime(all_articles['日期'])\n",
    "            all_articles = all_articles.sort_values(\n",
    "                ['作者', '日期'], ascending=[True, False])\n",
    "\n",
    "            # 按作者分組\n",
    "            for author in sorted(all_articles['作者'].unique()):\n",
    "                index_content.append(f\"## {author}\")\n",
    "                author_articles = all_articles[all_articles['作者'] == author]\n",
    "\n",
    "                # 按年份分組\n",
    "                for year in sorted(author_articles['日期'].dt.year.unique(), reverse=True):\n",
    "                    index_content.append(f\"\\n### {year}年\")\n",
    "                    year_articles = author_articles[author_articles['日期'].dt.year == year]\n",
    "\n",
    "                    # 生成文章列表\n",
    "                    for _, article in year_articles.iterrows():\n",
    "                        title = re.sub(r'[<>:\"/\\\\|?*]', '',\n",
    "                                       article['標題'])[:100]\n",
    "                        file_name = f\"{article['文章編號']}_{title}.md\"\n",
    "                        date_str = article['日期'].strftime('%Y-%m-%d')\n",
    "                        index_content.append(\n",
    "                            f\"- {date_str} [{article['標題']\n",
    "                                             }](./articles/{file_name})\"\n",
    "                        )\n",
    "\n",
    "                index_content.append(\"\")  # 作者之間加入空行\n",
    "\n",
    "            # 保存更新後的目錄文件\n",
    "            with open(index_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(index_content))\n",
    "\n",
    "            self.logger.info(f\"已成功更新文章目錄，新增 {len(new_articles)} 篇文章\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"更新目錄失敗: {str(e)}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"執行文章更新\"\"\"\n",
    "        self.logger.info(f\"開始檢查文章更新 (檢查範圍: {self.start_date.date()} 到 {self.end_date.date()})\")\n",
    "\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                futures = []\n",
    "\n",
    "                # 從期刊頁面獲取最新文章\n",
    "                max_page = self.get_max_page_number()\n",
    "                self.logger.info(f\"檢測到期刊總頁數: {max_page}\")\n",
    "\n",
    "                article_numbers = set()\n",
    "\n",
    "                # 只檢查前5頁以提高效率\n",
    "                for page_no in range(1, min(6, max_page + 1)):\n",
    "                    self.logger.info(f\"正在檢查第 {page_no} 頁的文章列表\")\n",
    "                    page_articles = self.get_article_urls_from_journal(page_no)\n",
    "                    article_numbers.update(page_articles)\n",
    "\n",
    "                    if not page_articles:\n",
    "                        self.logger.info(f\"第 {page_no} 頁沒有找到新文章，停止檢查\")\n",
    "                        break\n",
    "\n",
    "                self.logger.info(f\"找到 {len(article_numbers)} 篇待檢查文章\")\n",
    "\n",
    "                # 處理找到的文章\n",
    "                for article_no in article_numbers:\n",
    "                    if str(article_no) not in self.processed_articles:\n",
    "                        futures.append(executor.submit(self.fetch_article, article_no))\n",
    "\n",
    "                # 處理結果\n",
    "                with tqdm(total=len(futures), desc=\"更新文章\") as pbar:\n",
    "                    for future in futures:\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result:\n",
    "                                self.save_article(result)\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                fail_count += 1\n",
    "                            pbar.update(1)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"處理文章更新失敗: {str(e)}\")\n",
    "                            fail_count += 1\n",
    "                            pbar.update(1)\n",
    "\n",
    "        finally:\n",
    "            self.logger.info(f\"更新完成：成功 {success_count} 篇，失敗 {fail_count} 篇\")\n",
    "            if success_count > 0:\n",
    "                self.create_index()\n",
    "            self.logger.info(\"更新程序結束\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ArticleScraper(scan_mode=\"recent\")\n",
    "\n",
    "    # # 檢查特定文章\n",
    "    # specific_article = 913375\n",
    "    # scraper.check_specific_article(specific_article)\n",
    "\n",
    "    # 執行一般更新\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
