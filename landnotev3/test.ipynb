{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397bfbe5",
   "metadata": {},
   "source": [
    "# 最新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class JasperRealEstateScraper:\n",
    "    \"\"\"爬取 Jasper 不動產網站文章的爬蟲類\"\"\"\n",
    "\n",
    "    def __init__(self, base_url=\"https://www.jasper-realestate.com/posts/\", output_dir=\"scraped_data\"):\n",
    "        \"\"\"\n",
    "        初始化爬蟲\n",
    "        \n",
    "        Args:\n",
    "            base_url: 目標網站的基礎URL\n",
    "            output_dir: 輸出數據的目錄\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        self.output_dir = output_dir\n",
    "        self.results = []\n",
    "        self.current_page = 1\n",
    "\n",
    "        # 確保輸出目錄存在\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "    def random_sleep(self, min_seconds=2, max_seconds=5):\n",
    "        \"\"\"\n",
    "        隨機休息一段時間，避免過快請求\n",
    "        \n",
    "        Args:\n",
    "            min_seconds: 最小休息秒數\n",
    "            max_seconds: 最大休息秒數\n",
    "        \"\"\"\n",
    "        sleep_time = random.uniform(min_seconds, max_seconds)\n",
    "        print(f\"休息 {sleep_time:.2f} 秒...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    def get_page(self, url):\n",
    "        \"\"\"\n",
    "        獲取頁面內容\n",
    "        \n",
    "        Args:\n",
    "            url: 要獲取的頁面URL\n",
    "        \n",
    "        Returns:\n",
    "            BeautifulSoup 對象\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.encoding = 'utf-8'  # 確保正確解碼中文\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"獲取頁面時出錯: {url}, 錯誤: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_article_preview(self, article):\n",
    "        \"\"\"\n",
    "        從文章預覽中提取信息\n",
    "        \n",
    "        Args:\n",
    "            article: 包含文章預覽的 BeautifulSoup 對象\n",
    "        \n",
    "        Returns:\n",
    "            包含文章預覽信息的字典\n",
    "        \"\"\"\n",
    "        # 提取標題和連結\n",
    "        title_elem = article.find('h3', class_='elementor-post__title')\n",
    "        title = title_elem.find('a').text.strip(\n",
    "        ) if title_elem and title_elem.find('a') else 'N/A'\n",
    "        link = title_elem.find(\n",
    "            'a')['href'] if title_elem and title_elem.find('a') else 'N/A'\n",
    "\n",
    "        # 提取摘要和條文依據\n",
    "        excerpt_elem = article.find('div', class_='elementor-post__excerpt')\n",
    "        if excerpt_elem:\n",
    "            paragraphs = excerpt_elem.find_all('p')\n",
    "            excerpt = paragraphs[0].text.strip() if paragraphs else 'N/A'\n",
    "            legal_basis = paragraphs[1].text.strip() if len(\n",
    "                paragraphs) > 1 else 'N/A'\n",
    "        else:\n",
    "            excerpt, legal_basis = 'N/A', 'N/A'\n",
    "\n",
    "        # 提取發布日期\n",
    "        date_elem = article.find('span', class_='elementor-post-date')\n",
    "        date = date_elem.text.strip() if date_elem else 'N/A'\n",
    "\n",
    "        # 提取縮圖 URL\n",
    "        thumbnail_elem = article.find(\n",
    "            'div', class_='elementor-post__thumbnail')\n",
    "        thumbnail_url = 'N/A'\n",
    "        if thumbnail_elem:\n",
    "            img = thumbnail_elem.find('img')\n",
    "            # 優先從 data-lazy-src 提取，若無則從 src 提取\n",
    "            thumbnail_url = img.get('data-lazy-src') or img.get('src') or 'N/A'\n",
    "            # 清理 SVG 占位符\n",
    "            if 'data:image/svg+xml' in thumbnail_url:\n",
    "                thumbnail_url = img.get('data-lazy-src') or 'N/A'\n",
    "\n",
    "        # 提取標籤\n",
    "        badge_elem = article.find('div', class_='elementor-post__badge')\n",
    "        badge = badge_elem.text.strip() if badge_elem else 'N/A'\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'excerpt': excerpt,\n",
    "            'legal_basis': legal_basis,\n",
    "            'date': date,\n",
    "            'thumbnail_url': thumbnail_url,\n",
    "            'badge': badge\n",
    "        }\n",
    "\n",
    "    def extract_full_content(self, article_url):\n",
    "        \"\"\"\n",
    "        從文章頁面提取完整內容\n",
    "        \n",
    "        Args:\n",
    "            article_url: 文章頁面URL\n",
    "        \n",
    "        Returns:\n",
    "            文章的完整內容\n",
    "        \"\"\"\n",
    "        if article_url == 'N/A':\n",
    "            return \"無法獲取完整內容，鏈接不可用\"\n",
    "\n",
    "        try:\n",
    "            print(f\"正在抓取文章內容: {article_url}\")\n",
    "            article_soup = self.get_page(article_url)\n",
    "\n",
    "            if not article_soup:\n",
    "                return \"無法獲取文章頁面\"\n",
    "\n",
    "            # 根據HTML結構，精確定位文章內容\n",
    "            content_section = article_soup.find(\n",
    "                'div', class_='elementor-widget-theme-post-content')\n",
    "\n",
    "            if content_section:\n",
    "                # 提取文章內容\n",
    "                content_container = content_section.find(\n",
    "                    'div', class_='elementor-widget-container')\n",
    "\n",
    "                if content_container:\n",
    "                    # 提取所有標題和段落\n",
    "                    content_elements = content_container.find_all(\n",
    "                        ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol', 'li'])\n",
    "\n",
    "                    # 組織內容，保留標題層級結構\n",
    "                    full_content = \"\"\n",
    "                    for elem in content_elements:\n",
    "                        # 檢查是否是延伸閱讀或免費諮詢部分\n",
    "                        if elem.get_text().strip() in [\"．延伸閱讀\", \"．免費諮詢\", \"延伸閱讀\", \"免費諮詢\"]:\n",
    "                            # 如果找到延伸閱讀或免費諮詢，則停止處理\n",
    "                            break\n",
    "\n",
    "                        if elem.name.startswith('h'):  # 如果是標題\n",
    "                            full_content += f\"\\n\\n## {elem.get_text().strip()}\\n\\n\"\n",
    "                        elif elem.name == 'p':  # 如果是段落\n",
    "                            # 檢查段落是否為空\n",
    "                            text = elem.get_text().strip()\n",
    "                            if text:\n",
    "                                full_content += f\"{text}\\n\\n\"\n",
    "                        elif elem.name in ['ul', 'ol']:  # 如果是列表\n",
    "                            for li in elem.find_all('li'):\n",
    "                                full_content += f\"- {li.get_text().strip()}\\n\"\n",
    "                            full_content += \"\\n\"\n",
    "                        # 單獨的列表項\n",
    "                        elif elem.name == 'li' and not elem.parent.name in ['ul', 'ol']:\n",
    "                            full_content += f\"- {elem.get_text().strip()}\\n\"\n",
    "\n",
    "                    # 清理多餘的空行\n",
    "                    full_content = re.sub(\n",
    "                        r'\\n{3,}', '\\n\\n', full_content).strip()\n",
    "                else:\n",
    "                    # 如果找不到特定容器，則獲取整個內容區域的文本\n",
    "                    full_content = content_section.get_text(\n",
    "                        separator=\"\\n\").strip()\n",
    "\n",
    "                    # 移除延伸閱讀和免費諮詢部分\n",
    "                    sections_to_remove = [\"．延伸閱讀\", \"．免費諮詢\", \"延伸閱讀\", \"免費諮詢\"]\n",
    "                    for section in sections_to_remove:\n",
    "                        if section in full_content:\n",
    "                            full_content = full_content.split(section)[\n",
    "                                0].strip()\n",
    "            else:\n",
    "                # 如果找不到主要內容區域，嘗試其他可能的選擇器\n",
    "                full_content = \"無法找到主要內容區域，嘗試其他方法...\"\n",
    "\n",
    "                # 嘗試查找任何可能包含文章內容的區域\n",
    "                possible_content_areas = article_soup.find_all(\n",
    "                    'div', class_=['elementor-widget-container', 'entry-content', 'post-content'])\n",
    "\n",
    "                if possible_content_areas:\n",
    "                    # 選擇最長的內容區域作為可能的文章內容\n",
    "                    longest_content = \"\"\n",
    "                    for area in possible_content_areas:\n",
    "                        content = area.get_text(separator=\"\\n\").strip()\n",
    "                        if len(content) > len(longest_content):\n",
    "                            longest_content = content\n",
    "\n",
    "                    # 移除延伸閱讀和免費諮詢部分\n",
    "                    sections_to_remove = [\"．延伸閱讀\", \"．免費諮詢\", \"延伸閱讀\", \"免費諮詢\"]\n",
    "                    for section in sections_to_remove:\n",
    "                        if section in longest_content:\n",
    "                            longest_content = longest_content.split(section)[\n",
    "                                0].strip()\n",
    "\n",
    "                    full_content = longest_content\n",
    "                else:\n",
    "                    full_content = \"無法獲取文章內容\"\n",
    "\n",
    "            return full_content\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"獲取內容時出錯: {str(e)}\"\n",
    "\n",
    "    def has_next_page(self, soup):\n",
    "        \"\"\"\n",
    "        檢查是否有下一頁\n",
    "        \n",
    "        Args:\n",
    "            soup: 當前頁面的 BeautifulSoup 對象\n",
    "        \n",
    "        Returns:\n",
    "            布爾值，表示是否有下一頁\n",
    "        \"\"\"\n",
    "        # 查找分頁導航元素\n",
    "        pagination = soup.find('nav', class_='elementor-pagination')\n",
    "        if not pagination:\n",
    "            return False\n",
    "\n",
    "        # 檢查是否有下一頁按鈕或連結\n",
    "        current_page_links = pagination.find_all('a', class_='page-numbers')\n",
    "        for link in current_page_links:\n",
    "            # 檢查是否有\"下一頁\"按鈕或更高的頁碼\n",
    "            if link.text.strip() == '下一頁' or link.text.strip() == '»' or (link.text.strip().isdigit() and int(link.text.strip()) > self.current_page):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_next_page_url(self, soup):\n",
    "        \"\"\"\n",
    "        獲取下一頁的URL\n",
    "        \n",
    "        Args:\n",
    "            soup: 當前頁面的 BeautifulSoup 對象\n",
    "        \n",
    "        Returns:\n",
    "            下一頁的URL，如果沒有下一頁則返回None\n",
    "        \"\"\"\n",
    "        # 查找分頁導航元素\n",
    "        pagination = soup.find('nav', class_='elementor-pagination')\n",
    "        if not pagination:\n",
    "            return None\n",
    "\n",
    "        # 尋找\"下一頁\"按鈕\n",
    "        next_page_link = pagination.find(\n",
    "            'a', string=['下一頁', '»']) or pagination.find('a', class_='next')\n",
    "        if next_page_link and 'href' in next_page_link.attrs:\n",
    "            return next_page_link['href']\n",
    "\n",
    "        # 如果沒有明確的\"下一頁\"按鈕，尋找比當前頁碼更高的頁碼連結\n",
    "        page_links = pagination.find_all('a', class_='page-numbers')\n",
    "        for link in page_links:\n",
    "            if link.text.strip().isdigit() and int(link.text.strip()) > self.current_page:\n",
    "                return link['href']\n",
    "\n",
    "        return None\n",
    "\n",
    "    def scrape_all_articles(self):\n",
    "        \"\"\"爬取所有文章\"\"\"\n",
    "        current_url = self.base_url\n",
    "        total_articles = 0\n",
    "\n",
    "        while current_url:\n",
    "            print(f\"\\n正在爬取第 {self.current_page} 頁: {current_url}\")\n",
    "            soup = self.get_page(current_url)\n",
    "\n",
    "            if not soup:\n",
    "                print(f\"無法獲取頁面: {current_url}\")\n",
    "                break\n",
    "\n",
    "            # 找到所有文章\n",
    "            articles = soup.find_all('article', class_='elementor-post')\n",
    "\n",
    "            if not articles:\n",
    "                print(f\"在頁面上找不到文章: {current_url}\")\n",
    "                break\n",
    "\n",
    "            print(f\"找到 {len(articles)} 篇文章\")\n",
    "\n",
    "            for index, article in enumerate(articles):\n",
    "                print(f\"正在處理第 {self.current_page} 頁的第 {index+1} 篇文章...\")\n",
    "\n",
    "                # 提取文章預覽信息\n",
    "                article_info = self.extract_article_preview(article)\n",
    "\n",
    "                # 隨機休息，避免過快請求\n",
    "                self.random_sleep()\n",
    "\n",
    "                # 獲取完整內容\n",
    "                full_content = self.extract_full_content(article_info['link'])\n",
    "\n",
    "                # 添加完整內容到文章信息中\n",
    "                article_info['full_content'] = full_content\n",
    "\n",
    "                # 添加到結果列表\n",
    "                self.results.append(article_info)\n",
    "\n",
    "                print(f\"已抓取文章: {article_info['title']}\")\n",
    "                total_articles += 1\n",
    "\n",
    "                # 每抓取5篇文章保存一次結果，防止中途中斷丟失數據\n",
    "                if total_articles % 5 == 0:\n",
    "                    self.save_results(\n",
    "                        f\"jasper_articles_partial_{total_articles}.json\")\n",
    "\n",
    "                # 隨機休息，避免過快請求\n",
    "                self.random_sleep()\n",
    "\n",
    "            # 檢查是否有下一頁\n",
    "            if self.has_next_page(soup):\n",
    "                next_page_url = self.get_next_page_url(soup)\n",
    "                if next_page_url:\n",
    "                    current_url = next_page_url\n",
    "                    self.current_page += 1\n",
    "                    # 頁面之間的休息時間更長，避免被檢測\n",
    "                    self.random_sleep(5, 10)\n",
    "                else:\n",
    "                    print(\"找不到下一頁的URL\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"沒有更多頁面\")\n",
    "                break\n",
    "\n",
    "        print(f\"\\n爬取完成，共獲取 {len(self.results)} 篇文章\")\n",
    "        return self.results\n",
    "\n",
    "    def save_results(self, filename=None):\n",
    "        \"\"\"\n",
    "        保存爬取結果\n",
    "        \n",
    "        Args:\n",
    "            filename: 保存的文件名，如果為None則自動生成\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"jasper_articles_{timestamp}.json\"\n",
    "\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"結果已保存至 {filepath}\")\n",
    "\n",
    "\n",
    "# 執行爬蟲\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = JasperRealEstateScraper()\n",
    "    scraper.scrape_all_articles()\n",
    "    scraper.save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
