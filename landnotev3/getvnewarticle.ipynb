{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可以更新最近上傳的內容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 12:00:21,185 - INFO - 初始化更新模式: recent\n",
      "2025-04-19 12:00:21,188 - INFO - 開始檢查特定文章: 913706\n",
      "2025-04-19 12:00:21,190 - INFO - 請求 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913706\n",
      "2025-04-19 12:00:21,740 - INFO - 文章 913706 請求狀態碼: 200\n",
      "2025-04-19 12:00:21,742 - INFO - 已保存原始 HTML 到: real_estate_articles\\logs\\debug_article_913706.html\n",
      "2025-04-19 12:00:21,764 - INFO - 頁面 meta 信息: {'description': '蘇偉強,信託契約,自由處分,委託人,信託法,甲乙丙三人共有一筆A地，每人應有部分為1/ 3，由於丙具有地政士專業，因而三人協議將該A地共同信託給丙，並簽訂信託,高點來勝不動產,提供不動產經紀人,不動產估價師,地政士,高普考地政,地方特考地政等最新考試訊息!', 'keywords': '蘇偉強,信託契約,自由處分,委託人,信託法,不動產經紀人,不動產估價師,地政士,高考地政,普考地政,地方特考地政,許文昌,不動產證照,', 'viewport': 'width=device-width,initial-scale=1', 'robots': 'index,follow', 'citation_title': '共有土地協議信託給共有人之一,曾榮耀老師', 'citation_author': '蘇偉強', 'citation_publication_date': '2025/04/01', 'citation_journal_t??itle': '許文昌/曾榮耀不動產全制霸', 'citation_firstpage': '1', 'citation_lastpage': '1'}\n",
      "2025-04-19 12:00:21,768 - INFO - Meta 標籤中的作者: 蘇偉強\n",
      "2025-04-19 12:00:21,769 - ERROR - 無法抓取文章 913706\n",
      "2025-04-19 12:00:21,770 - INFO - 開始檢查特定文章: 913623\n",
      "2025-04-19 12:00:21,771 - INFO - 請求 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913623\n",
      "2025-04-19 12:00:21,853 - INFO - 文章 913623 請求狀態碼: 200\n",
      "2025-04-19 12:00:21,855 - INFO - 已保存原始 HTML 到: real_estate_articles\\logs\\debug_article_913623.html\n",
      "2025-04-19 12:00:21,879 - INFO - 頁面 meta 信息: {'description': '蘇偉強,共有人,繼承人,繼承登記,默示分管,今日專欄舉一篇有關共有人占用共有土地之默示分管關係案例供參\\r\\n甲有一筆位於某市的A地，其死亡後由1,高點來勝不動產,提供不動產經紀人,不動產估價師,地政士,高普考地政,地方特考地政等最新考試訊息!', 'keywords': '蘇偉強,共有人,繼承人,繼承登記,默示分管,不動產經紀人,不動產估價師,地政士,高考地政,普考地政,地方特考地政,許文昌,不動產證照,', 'viewport': 'width=device-width,initial-scale=1', 'robots': 'index,follow', 'citation_title': '共有人占用共有土地之默示分管關係,曾榮耀老師', 'citation_author': '蘇偉強', 'citation_publication_date': '2025/03/01', 'citation_journal_t??itle': '許文昌/曾榮耀不動產全制霸', 'citation_firstpage': '1', 'citation_lastpage': '1'}\n",
      "2025-04-19 12:00:21,880 - INFO - Meta 標籤中的作者: 蘇偉強\n",
      "2025-04-19 12:00:21,882 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913623\n",
      "2025-04-19 12:00:22,013 - INFO - 文章 913623 請求狀態碼: 200\n",
      "2025-04-19 12:00:22,014 - INFO - 文章 913623 響應內容長度: 30800\n",
      "2025-04-19 12:00:22,017 - INFO - 已保存文章 913623 的原始響應到: real_estate_articles\\logs\\article_913623_response.html\n",
      "2025-04-19 12:00:22,043 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:22,048 - INFO - 文章 913623 找到 9 個資料列\n",
      "2025-04-19 12:00:22,052 - INFO - 文章 913623 成功解析 7 個欄位\n",
      "2025-04-19 12:00:22,054 - INFO - 文章 913623 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:22,057 - INFO - 文章 913623 解析成功\n",
      "2025-04-19 12:00:22,058 - INFO - 文章 913623 抓取成功，開始保存\n",
      "2025-04-19 12:00:22,811 - INFO - 文章 913623 已保存到 real_estate_articles\\articles\\913623_共有人占用共有土地之默示分管關係,曾榮耀老師.md\n",
      "2025-04-19 12:00:23,198 - INFO - 已成功更新文章目錄，新增 1 篇文章\n",
      "2025-04-19 12:00:23,200 - INFO - 成功抓取並保存文章 913623\n",
      "2025-04-19 12:00:23,202 - INFO - 開始檢查特定文章: 913646\n",
      "2025-04-19 12:00:23,204 - INFO - 請求 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913646\n",
      "2025-04-19 12:00:23,253 - INFO - 文章 913646 請求狀態碼: 200\n",
      "2025-04-19 12:00:23,255 - INFO - 已保存原始 HTML 到: real_estate_articles\\logs\\debug_article_913646.html\n",
      "2025-04-19 12:00:23,277 - INFO - 頁面 meta 信息: {'description': '蘇偉強,信託,受益權,受託人,受益人,案例\\r\\n老張信託其所有之不動產給小美，約定受託人小美基於老張之女,高點來勝不動產,提供不動產經紀人,不動產估價師,地政士,高普考地政,地方特考地政等最新考試訊息!', 'keywords': '蘇偉強,信託,受益權,受託人,受益人,不動產經紀人,不動產估價師,地政士,高考地政,普考地政,地方特考地政,許文昌,不動產證照,', 'viewport': 'width=device-width,initial-scale=1', 'robots': 'index,follow', 'citation_title': '信託受益人死亡之處理方式,曾榮耀老師', 'citation_author': '蘇偉強', 'citation_publication_date': '2025/04/01', 'citation_journal_t??itle': '許文昌/曾榮耀不動產全制霸', 'citation_firstpage': '1', 'citation_lastpage': '1'}\n",
      "2025-04-19 12:00:23,278 - INFO - Meta 標籤中的作者: 蘇偉強\n",
      "2025-04-19 12:00:23,279 - ERROR - 無法抓取文章 913646\n",
      "2025-04-19 12:00:23,279 - INFO - 開始檢查文章更新\n",
      "2025-04-19 12:00:24,210 - INFO - 檢測到期刊總頁數: 1\n",
      "2025-04-19 12:00:24,211 - INFO - 正在檢查第 1 頁的文章列表\n",
      "2025-04-19 12:00:25,754 - INFO - 第 1 頁找到 7 篇新文章: [913666, 913574, 913542, 913481, 913455, 913430, 913419]\n",
      "2025-04-19 12:00:25,756 - INFO - 找到 7 篇待檢查文章: [913419, 913430, 913455, 913481, 913542, 913574, 913666]\n",
      "更新文章:   0%|          | 0/7 [00:00<?, ?it/s]2025-04-19 12:00:26,334 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913542\n",
      "2025-04-19 12:00:26,391 - INFO - 文章 913542 請求狀態碼: 200\n",
      "2025-04-19 12:00:26,392 - INFO - 文章 913542 響應內容長度: 30911\n",
      "2025-04-19 12:00:26,395 - INFO - 已保存文章 913542 的原始響應到: real_estate_articles\\logs\\article_913542_response.html\n",
      "2025-04-19 12:00:26,427 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:26,434 - INFO - 文章 913542 找到 9 個資料列\n",
      "2025-04-19 12:00:26,439 - INFO - 文章 913542 成功解析 7 個欄位\n",
      "2025-04-19 12:00:26,441 - INFO - 文章 913542 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:26,445 - INFO - 文章 913542 解析成功\n",
      "2025-04-19 12:00:26,446 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913574\n",
      "2025-04-19 12:00:26,496 - INFO - 文章 913574 請求狀態碼: 200\n",
      "2025-04-19 12:00:26,498 - INFO - 文章 913574 響應內容長度: 31648\n",
      "2025-04-19 12:00:26,500 - INFO - 已保存文章 913574 的原始響應到: real_estate_articles\\logs\\article_913574_response.html\n",
      "2025-04-19 12:00:26,540 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:26,546 - INFO - 文章 913574 找到 9 個資料列\n",
      "2025-04-19 12:00:26,549 - INFO - 文章 913574 成功解析 7 個欄位\n",
      "2025-04-19 12:00:26,550 - INFO - 文章 913574 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:26,553 - INFO - 文章 913574 解析成功\n",
      "2025-04-19 12:00:26,566 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913666\n",
      "2025-04-19 12:00:26,579 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913481\n",
      "2025-04-19 12:00:26,599 - INFO - 文章 913666 請求狀態碼: 200\n",
      "2025-04-19 12:00:26,600 - INFO - 文章 913666 響應內容長度: 32218\n",
      "2025-04-19 12:00:26,602 - INFO - 已保存文章 913666 的原始響應到: real_estate_articles\\logs\\article_913666_response.html\n",
      "2025-04-19 12:00:26,629 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:26,662 - INFO - 文章 913666 找到 9 個資料列\n",
      "2025-04-19 12:00:26,665 - INFO - 文章 913666 成功解析 7 個欄位\n",
      "2025-04-19 12:00:26,666 - INFO - 文章 913666 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:26,670 - INFO - 文章 913666 解析成功\n",
      "2025-04-19 12:00:27,282 - INFO - 文章 913481 請求狀態碼: 200\n",
      "2025-04-19 12:00:27,283 - INFO - 文章 913481 響應內容長度: 30677\n",
      "2025-04-19 12:00:27,285 - INFO - 已保存文章 913481 的原始響應到: real_estate_articles\\logs\\article_913481_response.html\n",
      "2025-04-19 12:00:27,312 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:27,315 - INFO - 文章 913481 找到 9 個資料列\n",
      "2025-04-19 12:00:27,317 - INFO - 文章 913481 成功解析 7 個欄位\n",
      "2025-04-19 12:00:27,318 - INFO - 文章 913481 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:27,320 - INFO - 文章 913481 解析成功\n",
      "2025-04-19 12:00:27,571 - INFO - 文章 913666 已保存到 real_estate_articles\\articles\\913666_113年憲判字第11號有關遺贈稅法尚未修法前之因應方式,曾榮耀老師.md\n",
      "更新文章:  14%|█▍        | 1/7 [00:01<00:10,  1.73s/it]2025-04-19 12:00:27,777 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913419\n",
      "2025-04-19 12:00:27,818 - INFO - 文章 913419 請求狀態碼: 200\n",
      "2025-04-19 12:00:27,824 - INFO - 文章 913419 響應內容長度: 30548\n",
      "2025-04-19 12:00:27,834 - INFO - 已保存文章 913419 的原始響應到: real_estate_articles\\logs\\article_913419_response.html\n",
      "2025-04-19 12:00:27,871 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:27,883 - INFO - 文章 913419 找到 9 個資料列\n",
      "2025-04-19 12:00:27,891 - INFO - 文章 913419 成功解析 7 個欄位\n",
      "2025-04-19 12:00:27,894 - INFO - 文章 913419 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:27,899 - INFO - 文章 913419 解析成功\n",
      "2025-04-19 12:00:28,010 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913455\n",
      "2025-04-19 12:00:28,168 - INFO - 開始請求文章 URL: https://real-estate.get.com.tw/Columns/detail.aspx?no=913430\n",
      "2025-04-19 12:00:28,237 - INFO - 文章 913455 請求狀態碼: 200\n",
      "2025-04-19 12:00:28,238 - INFO - 文章 913455 響應內容長度: 31440\n",
      "2025-04-19 12:00:28,241 - INFO - 已保存文章 913455 的原始響應到: real_estate_articles\\logs\\article_913455_response.html\n",
      "2025-04-19 12:00:28,244 - INFO - 文章 913430 請求狀態碼: 200\n",
      "2025-04-19 12:00:28,278 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:28,279 - INFO - 文章 913430 響應內容長度: 30797\n",
      "2025-04-19 12:00:28,287 - INFO - 文章 913455 找到 9 個資料列\n",
      "2025-04-19 12:00:28,292 - INFO - 已保存文章 913430 的原始響應到: real_estate_articles\\logs\\article_913430_response.html\n",
      "2025-04-19 12:00:28,295 - INFO - 文章 913455 成功解析 7 個欄位\n",
      "2025-04-19 12:00:28,331 - INFO - 從 meta 標籤找到文章資訊\n",
      "2025-04-19 12:00:28,332 - INFO - 文章 913455 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:28,336 - INFO - 文章 913430 找到 9 個資料列\n",
      "2025-04-19 12:00:28,342 - INFO - 文章 913455 解析成功\n",
      "2025-04-19 12:00:28,345 - INFO - 文章 913430 成功解析 7 個欄位\n",
      "2025-04-19 12:00:28,347 - INFO - 文章 913430 實際作者: 蘇偉強\n",
      "2025-04-19 12:00:28,350 - INFO - 文章 913430 解析成功\n",
      "2025-04-19 12:00:28,544 - INFO - 文章 913542 已保存到 real_estate_articles\\articles\\913542_房地合一稅計算範例-受贈後出售,曾榮耀老師.md\n",
      "更新文章:  29%|██▊       | 2/7 [00:02<00:06,  1.28s/it]2025-04-19 12:00:29,374 - INFO - 文章 913574 已保存到 real_estate_articles\\articles\\913574_個人墓地出售是否得主張優先購買之疑義,曾榮耀老師.md\n",
      "更新文章:  43%|████▎     | 3/7 [00:03<00:04,  1.08s/it]2025-04-19 12:00:30,148 - INFO - 文章 913481 已保存到 real_estate_articles\\articles\\913481_共有人占用共有土地之爭議,曾榮耀老師.md\n",
      "更新文章:  57%|█████▋    | 4/7 [00:04<00:02,  1.04it/s]2025-04-19 12:00:30,991 - INFO - 文章 913419 已保存到 real_estate_articles\\articles\\913419_基地優先購買權之要件探討－合法建物,曾榮耀老師.md\n",
      "更新文章:  71%|███████▏  | 5/7 [00:05<00:01,  1.09it/s]2025-04-19 12:00:31,846 - INFO - 文章 913455 已保存到 real_estate_articles\\articles\\913455_時效取得地上權之主觀意思要件探討,曾榮耀老師.md\n",
      "更新文章:  86%|████████▌ | 6/7 [00:06<00:00,  1.12it/s]2025-04-19 12:00:32,657 - INFO - 文章 913430 已保存到 real_estate_articles\\articles\\913430_受託人得否將信託財產設定抵押權,曾榮耀老師.md\n",
      "更新文章: 100%|██████████| 7/7 [00:06<00:00,  1.03it/s]\n",
      "2025-04-19 12:00:32,662 - INFO - 更新完成：成功 7 篇，失敗 0 篇\n",
      "2025-04-19 12:00:33,056 - INFO - 已成功更新文章目錄，新增 7 篇文章\n",
      "2025-04-19 12:00:33,058 - INFO - 更新程序結束\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "import urllib3\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class ArticleScraper:\n",
    "    def __init__(self, scan_mode=\"recent\", data_file=\"articles.xlsx\"):\n",
    "        \"\"\"初始化爬蟲設定\"\"\"\n",
    "        # 常量設定\n",
    "        self.SETTINGS = {\n",
    "            'BASE_URL': \"https://real-estate.get.com.tw/Columns/\",\n",
    "            'TARGET_AUTHORS': [\"曾榮耀\", \"許文昌\", \"蘇偉強\"],\n",
    "            'JOURNAL_PARAMS': {\n",
    "                \"no\": \"1282\",\n",
    "                \"pno\": \"51120\"  # 根據你的例子，使用 pno=51120\n",
    "            },\n",
    "            'PERFORMANCE': {\n",
    "                'BATCH_SIZE': 50,\n",
    "                'MAX_WORKERS': 4,\n",
    "                'MAX_RETRIES': 5,\n",
    "                'RETRY_DELAY': 3,\n",
    "                'REQUEST_INTERVAL': 2  # 增加間隔以避免網站限制\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 基本設定\n",
    "        self.base_url = self.SETTINGS['BASE_URL']\n",
    "        self.detail_url = f\"{self.base_url}detail.aspx\"\n",
    "        self.journal_url = f\"{self.base_url}journal.aspx\"\n",
    "        self.target_authors = self.SETTINGS['TARGET_AUTHORS']\n",
    "        self.scan_mode = scan_mode\n",
    "        self.data_file = Path(data_file)\n",
    "\n",
    "        # 期刊參數設定\n",
    "        self.journal_params = self.SETTINGS['JOURNAL_PARAMS']\n",
    "\n",
    "        # 效能設定\n",
    "        self.batch_size = self.SETTINGS['PERFORMANCE']['BATCH_SIZE']\n",
    "        self.max_workers = self.SETTINGS['PERFORMANCE']['MAX_WORKERS']\n",
    "        self.max_retries = self.SETTINGS['PERFORMANCE']['MAX_RETRIES']\n",
    "        self.retry_delay = self.SETTINGS['PERFORMANCE']['RETRY_DELAY']\n",
    "        self.request_interval = self.SETTINGS['PERFORMANCE']['REQUEST_INTERVAL']\n",
    "\n",
    "        # 初始化其他組件\n",
    "        self.setup_directories()\n",
    "        self.setup_session()\n",
    "        self.setup_logger()\n",
    "        self.processed_articles = set()\n",
    "        self.load_processed_articles()\n",
    "        self.last_request_time = 0\n",
    "\n",
    "        # 記錄掃描設定到日誌\n",
    "        self.logger.info(f\"初始化更新模式: {scan_mode}\")\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"建立必要的目錄結構\"\"\"\n",
    "        self.base_dir = Path(\"real_estate_articles\")\n",
    "        self.articles_dir = self.base_dir / \"articles\"\n",
    "        self.images_dir = self.articles_dir / \"images\"\n",
    "        self.logs_dir = self.base_dir / \"logs\"\n",
    "\n",
    "        for directory in [self.base_dir, self.articles_dir, self.images_dir, self.logs_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 創建預設的失敗圖片\n",
    "        self.failed_image_path = self.images_dir / \"image_download_failed.png\"\n",
    "        if not self.failed_image_path.exists():\n",
    "            try:\n",
    "                from PIL import Image, ImageDraw\n",
    "                img = Image.new('RGB', (400, 100), color='white')\n",
    "                d = ImageDraw.Draw(img)\n",
    "                d.text((10, 40), \"Image Download Failed\", fill='black')\n",
    "                img.save(self.failed_image_path)\n",
    "            except Exception:\n",
    "                self.failed_image_path.touch()\n",
    "\n",
    "    def setup_session(self):\n",
    "        \"\"\"設定請求session\"\"\"\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def setup_logger(self):\n",
    "        \"\"\"設定日誌系統\"\"\"\n",
    "        self.logger = logging.getLogger('ArticleScraper')\n",
    "        self.logger.handlers = []\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        log_file = self.logs_dir / f\"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        handlers = [\n",
    "            logging.FileHandler(log_file, encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        for handler in handlers:\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def load_processed_articles(self):\n",
    "        \"\"\"載入已處理的文章\"\"\"\n",
    "        if self.data_file.exists():\n",
    "            df = pd.read_excel(self.data_file)\n",
    "            if '文章編號' in df.columns:\n",
    "                self.processed_articles = set(df['文章編號'].astype(str))\n",
    "\n",
    "    def wait_between_requests(self):\n",
    "        \"\"\"控制請求間隔\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_request_time\n",
    "        if elapsed < self.request_interval:\n",
    "            time.sleep(self.request_interval - elapsed + random.uniform(0, 0.5))\n",
    "        self.last_request_time = current_time\n",
    "\n",
    "    def get_max_page_number(self) -> int:\n",
    "        \"\"\"獲取期刊最大頁數\"\"\"\n",
    "        try:\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=self.journal_params,\n",
    "                timeout=30,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            pagination = soup.select('.pagination a')\n",
    "            if pagination:\n",
    "                page_numbers = []\n",
    "                for a in pagination:\n",
    "                    try:\n",
    "                        page_numbers.append(int(a.text.strip()))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                max_page = max(page_numbers) if page_numbers else 1\n",
    "                self.logger.info(f\"檢測到最大頁數: {max_page}\")\n",
    "                return max_page\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取最大頁數失敗: {str(e)}\")\n",
    "            return 50  # 設置較大預設值以確保不遺漏\n",
    "\n",
    "    def get_article_urls_from_journal(self, page_no: int) -> List[int]:\n",
    "        \"\"\"從期刊頁面獲取文章編號列表\"\"\"\n",
    "        try:\n",
    "            params = self.journal_params.copy()\n",
    "            params['page_no'] = page_no\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=params,\n",
    "                timeout=30,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            articles = []\n",
    "            for link in soup.select('a[href*=\"detail.aspx?no=\"]'):\n",
    "                try:\n",
    "                    href = link.get('href', '')\n",
    "                    if match := re.search(r'no=(\\d+)', href):\n",
    "                        article_no = int(match.group(1))\n",
    "                        if str(article_no) not in self.processed_articles:\n",
    "                            articles.append(article_no)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            if articles:\n",
    "                self.logger.info(f\"第 {page_no} 頁找到 {len(articles)} 篇新文章: {articles}\")\n",
    "            else:\n",
    "                self.logger.info(f\"第 {page_no} 頁沒有新文章\")\n",
    "            return articles\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取第 {page_no} 頁文章列表失敗: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_article(self, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"抓取單篇文章\"\"\"\n",
    "        if str(article_no) in self.processed_articles:\n",
    "            self.logger.debug(f\"文章 {article_no} 已處理過，跳過\")\n",
    "            return None\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                self.wait_between_requests()\n",
    "                url = f\"{self.detail_url}?no={article_no}\"\n",
    "                self.logger.info(f\"開始請求文章 URL: {url}\")\n",
    "                response = self.session.get(url, timeout=30, verify=False)\n",
    "                self.logger.info(f\"文章 {article_no} 請求狀態碼: {response.status_code}\")\n",
    "                if response.status_code == 404:\n",
    "                    self.logger.error(f\"文章 {article_no} 不存在 (404)\")\n",
    "                    return None\n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "                content_length = len(response.text)\n",
    "                self.logger.info(f\"文章 {article_no} 響應內容長度: {content_length}\")\n",
    "                if content_length < 100:\n",
    "                    self.logger.error(f\"文章 {article_no} 響應內容過短，可能是無效響應\")\n",
    "                    return None\n",
    "                debug_file = self.logs_dir / f\"article_{article_no}_response.html\"\n",
    "                with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                self.logger.info(f\"已保存文章 {article_no} 的原始響應到: {debug_file}\")\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                if not soup.select('.columnsDetail_tableRow'):\n",
    "                    self.logger.error(f\"文章 {article_no} 頁面結構不符合預期，未找到 .columnsDetail_tableRow\")\n",
    "                    return None\n",
    "                article_data = self.parse_article(soup, article_no)\n",
    "                if article_data and self.validate_article(article_data):\n",
    "                    self.logger.info(f\"文章 {article_no} 解析成功\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    self.logger.error(f\"文章 {article_no} 解析失敗或不符合條件\")\n",
    "                    return None\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(f\"抓取文章 {article_no} 失敗 (嘗試 {retry + 1}/{self.max_retries}): {str(e)}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    wait_time = self.retry_delay * (retry + 1)\n",
    "                    self.logger.info(f\"等待 {wait_time} 秒後重試...\")\n",
    "                    time.sleep(wait_time)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"處理文章 {article_no} 時發生未預期錯誤: {str(e)}\")\n",
    "                import traceback\n",
    "                self.logger.error(f\"錯誤堆疊: {traceback.format_exc()}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    wait_time = self.retry_delay * (retry + 1)\n",
    "                    self.logger.info(f\"等待 {wait_time} 秒後重試...\")\n",
    "                    time.sleep(wait_time)\n",
    "                continue\n",
    "        self.logger.error(f\"文章 {article_no} 重試 {self.max_retries} 次後仍失敗\")\n",
    "        return None\n",
    "\n",
    "    def check_specific_article(self, article_no: int):\n",
    "        \"\"\"檢查特定文章\"\"\"\n",
    "        self.logger.info(f\"開始檢查特定文章: {article_no}\")\n",
    "        try:\n",
    "            url = f\"{self.detail_url}?no={article_no}\"\n",
    "            self.logger.info(f\"請求 URL: {url}\")\n",
    "            response = self.session.get(url, timeout=30, verify=False)\n",
    "            self.logger.info(f\"文章 {article_no} 請求狀態碼: {response.status_code}\")\n",
    "            debug_file = self.logs_dir / f\"debug_article_{article_no}.html\"\n",
    "            with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            self.logger.info(f\"已保存原始 HTML 到: {debug_file}\")\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            meta_tags = soup.find_all('meta')\n",
    "            meta_info = {}\n",
    "            for tag in meta_tags:\n",
    "                name = tag.get('name', '')\n",
    "                content = tag.get('content', '')\n",
    "                if name and content:\n",
    "                    meta_info[name] = content\n",
    "            self.logger.info(f\"頁面 meta 信息: {meta_info}\")\n",
    "            meta_author = soup.select_one('meta[name=\"citation_author\"]')\n",
    "            if meta_author:\n",
    "                author = meta_author.get('content', '')\n",
    "                self.logger.info(f\"Meta 標籤中的作者: {author}\")\n",
    "                author_match = any(target in author for target in self.target_authors)\n",
    "                if not author_match:\n",
    "                    self.logger.warning(f\"作者不在目標列表中: {author}\")\n",
    "            result = self.fetch_article(article_no)\n",
    "            if result:\n",
    "                self.logger.info(f\"文章 {article_no} 抓取成功，開始保存\")\n",
    "                self.save_article(result)\n",
    "                self.create_index()\n",
    "                self.logger.info(f\"成功抓取並保存文章 {article_no}\")\n",
    "            else:\n",
    "                self.logger.error(f\"無法抓取文章 {article_no}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"檢查文章 {article_no} 時發生錯誤: {str(e)}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"錯誤堆疊: {traceback.format_exc()}\")\n",
    "\n",
    "    def parse_article(self, soup: BeautifulSoup, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"解析文章內容\"\"\"\n",
    "        try:\n",
    "            article_info = {}\n",
    "            found_fields = 0\n",
    "            meta_author = soup.select_one('meta[name=\"citation_author\"]')\n",
    "            meta_title = soup.select_one('meta[name=\"citation_title\"]')\n",
    "            meta_date = soup.select_one('meta[name=\"citation_publication_date\"]')\n",
    "            if meta_author and meta_title and meta_date:\n",
    "                self.logger.info(f\"從 meta 標籤找到文章資訊\")\n",
    "                article_info['作者'] = meta_author.get('content', '').strip()\n",
    "                article_info['標題'] = meta_title.get('content', '').strip()\n",
    "                article_info['日期'] = meta_date.get('content', '').strip()\n",
    "                found_fields += 3\n",
    "            rows = soup.select('.columnsDetail_tableRow')\n",
    "            self.logger.info(f\"文章 {article_no} 找到 {len(rows)} 個資料列\")\n",
    "            for row in rows:\n",
    "                th = row.select_one('.columnsDetail_tableth')\n",
    "                td = row.select_one('.columnsDetail_tabletd')\n",
    "                if th and td:\n",
    "                    key = th.text.strip()\n",
    "                    value = td.text.strip()\n",
    "                    self.logger.debug(f\"文章 {article_no} 欄位: {key} = {value[:50]}...\")\n",
    "                    if key == '篇名':\n",
    "                        article_info['標題'] = value\n",
    "                        found_fields += 1\n",
    "                    elif key == '作者':\n",
    "                        article_info['作者'] = value\n",
    "                        found_fields += 1\n",
    "                    elif key == '日期':\n",
    "                        article_info['日期'] = value\n",
    "                        found_fields += 1\n",
    "                    elif key == '內文':\n",
    "                        article_info['內文HTML'] = str(td)\n",
    "                        found_fields += 1\n",
    "            self.logger.info(f\"文章 {article_no} 成功解析 {found_fields} 個欄位\")\n",
    "            missing_fields = []\n",
    "            for field in ['標題', '作者', '日期']:\n",
    "                if field not in article_info:\n",
    "                    missing_fields.append(field)\n",
    "            if missing_fields:\n",
    "                self.logger.error(f\"文章 {article_no} 缺少必要欄位: {', '.join(missing_fields)}\")\n",
    "                return None\n",
    "            author = article_info['作者']\n",
    "            self.logger.info(f\"文章 {article_no} 實際作者: {author}\")\n",
    "            author_match = any(target in author for target in self.target_authors)\n",
    "            if not author_match:\n",
    "                self.logger.warning(f\"文章 {article_no} 作者不符合目標: {author}\")\n",
    "                return None\n",
    "            if '內文HTML' not in article_info:\n",
    "                content_div = soup.select_one('.columnsDetail_tabletd#SearchItem')\n",
    "                if content_div:\n",
    "                    article_info['內文HTML'] = str(content_div)\n",
    "                    self.logger.info(\"從頁面中提取內文 HTML\")\n",
    "            content = self.process_content(article_info.get('內文HTML', ''), article_no)\n",
    "            if not content:\n",
    "                self.logger.error(f\"文章 {article_no} 內文處理後為空\")\n",
    "                return None\n",
    "            article_data = {\n",
    "                '文章編號': article_no,\n",
    "                '標題': article_info['標題'],\n",
    "                '作者': article_info['作者'],\n",
    "                '日期': article_info['日期'],\n",
    "                '內文': content,\n",
    "                'URL': f\"{self.detail_url}?no={article_no}\",\n",
    "                '爬取時間': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            return article_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"解析文章 {article_no} 失敗: {str(e)}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"錯誤堆疊: {traceback.format_exc()}\")\n",
    "            return None\n",
    "\n",
    "    def download_image(self, img_url: str, article_no: int) -> Optional[str]:\n",
    "        \"\"\"下載圖片並返回本地檔名\"\"\"\n",
    "        try:\n",
    "            if not img_url.startswith(('http://', 'https://')):\n",
    "                img_url = urllib.parse.urljoin(self.base_url, img_url)\n",
    "            url_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]\n",
    "            file_ext = os.path.splitext(img_url)[1] or '.jpg'\n",
    "            local_filename = f\"{article_no}_{url_hash}{file_ext}\"\n",
    "            local_path = self.images_dir / local_filename\n",
    "            if local_path.exists():\n",
    "                return local_filename\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(img_url, timeout=30, verify=False)\n",
    "            response.raise_for_status()\n",
    "            with open(local_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return local_filename\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"下載圖片失敗 ({img_url}): {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_content(self, html_content: str, article_no: int) -> str:\n",
    "        \"\"\"處理文章內容，包括下載圖片和清理HTML\"\"\"\n",
    "        if not html_content:\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        image_references = []\n",
    "        for index, img in enumerate(soup.find_all('img'), 1):\n",
    "            img_url = img.get('src', '')\n",
    "            if img_url:\n",
    "                local_filename = self.download_image(img_url, article_no)\n",
    "                if local_filename:\n",
    "                    image_references.append(f\"\\n![圖片{index}](./images/{local_filename})\\n\")\n",
    "                    img.replace_with(f\"[圖片{index}]\")\n",
    "                else:\n",
    "                    img.replace_with(\"[圖片下載失敗]\")\n",
    "        content = self._format_content(soup)\n",
    "        if image_references:\n",
    "            content += \"\\n\\n## 文章圖片\\n\" + \"\".join(image_references)\n",
    "        return content\n",
    "\n",
    "    def _format_content(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"格式化文章內容，處理換行和縮排\"\"\"\n",
    "        allowed_tags = {'p', 'br', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li'}\n",
    "        for tag in soup.find_all():\n",
    "            if tag.name not in allowed_tags:\n",
    "                tag.unwrap()\n",
    "        for p in soup.find_all('p'):\n",
    "            text = p.get_text().strip()\n",
    "            if text:\n",
    "                p.string = ' '.join(text.split())\n",
    "                p.append('\\n\\n')\n",
    "        for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            level = int(h.name[1])\n",
    "            prefix = '#' * level + ' '\n",
    "            h.string = f'\\n{prefix}{h.get_text().strip()}\\n'\n",
    "        for li in soup.find_all('li'):\n",
    "            indent = '  '\n",
    "            if li.parent.name == 'ol':\n",
    "                index = len(li.find_previous_siblings('li')) + 1\n",
    "                li.insert(0, f'{indent}{index}. ')\n",
    "            else:\n",
    "                li.insert(0, f'{indent}• ')\n",
    "            li.append('\\n')\n",
    "        content = soup.get_text()\n",
    "        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "        content = re.sub(r'[ \\t]+', ' ', content)\n",
    "        content = re.sub(r' *\\n *', '\\n', content)\n",
    "        paragraphs = [p.strip() for p in content.split('\\n') if p.strip()]\n",
    "        formatted_content = '\\n\\n'.join(paragraphs)\n",
    "        return formatted_content\n",
    "\n",
    "    def validate_article(self, article_data: Dict) -> bool:\n",
    "        \"\"\"驗證文章資料完整性\"\"\"\n",
    "        required_fields = ['標題', '作者', '日期', '內文']\n",
    "        return all(field in article_data and article_data[field] for field in required_fields)\n",
    "\n",
    "    def save_article(self, article_data: Dict) -> None:\n",
    "        \"\"\"儲存文章\"\"\"\n",
    "        try:\n",
    "            new_df = pd.DataFrame([article_data])\n",
    "            if self.data_file.exists():\n",
    "                df = pd.read_excel(self.data_file)\n",
    "                df = pd.concat([df, new_df]).drop_duplicates(subset=['文章編號'])\n",
    "            else:\n",
    "                df = new_df\n",
    "            df.to_excel(self.data_file, index=False)\n",
    "            article_no = article_data['文章編號']\n",
    "            title = re.sub(r'[<>:\"/\\\\|?*]', '', article_data['標題'])[:100]\n",
    "            markdown_content = f\"\"\"# {article_data['標題']}\n",
    "\n",
    "## 文章資訊\n",
    "- 文章編號：{article_no}\n",
    "- 作者：{article_data['作者']}\n",
    "- 發布日期：{article_data['日期']}\n",
    "- 爬取時間：{article_data['爬取時間']}\n",
    "- 原文連結：[閱讀原文]({article_data['URL']})\n",
    "\n",
    "## 內文\n",
    "{article_data['內文']}\n",
    "\n",
    "---\n",
    "*注：本文圖片存放於 ./images/ 目錄下*\n",
    "\"\"\"\n",
    "            file_path = self.articles_dir / f\"{article_no}_{title}.md\"\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            self.processed_articles.add(str(article_no))\n",
    "            self.logger.info(f\"文章 {article_no} 已保存到 {file_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"儲存文章 {article_data.get('文章編號')} 失敗: {str(e)}\")\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"創建文章索引，整合原有目錄內容\"\"\"\n",
    "        try:\n",
    "            if not self.data_file.exists():\n",
    "                self.logger.info(\"未找到 articles.xlsx，無法創建索引\")\n",
    "                return\n",
    "            df = pd.read_excel(self.data_file)\n",
    "            index_path = self.base_dir / \"README.md\"\n",
    "            existing_content = []\n",
    "            existing_articles = set()\n",
    "            if index_path.exists():\n",
    "                with open(index_path, 'r', encoding='utf-8') as f:\n",
    "                    existing_content = f.read().splitlines()\n",
    "                    for line in existing_content:\n",
    "                        if match := re.search(r'/(\\d+)_[^/]+\\.md', line):\n",
    "                            existing_articles.add(match.group(1))\n",
    "            df['文章編號'] = df['文章編號'].astype(str)\n",
    "            new_articles = df[~df['文章編號'].isin(existing_articles)]\n",
    "            if new_articles.empty and existing_content:\n",
    "                self.logger.info(\"沒有新文章需要添加到目錄\")\n",
    "                return\n",
    "            index_content = []\n",
    "            for line in existing_content:\n",
    "                if line.startswith('## '):\n",
    "                    break\n",
    "                index_content.append(line)\n",
    "            if not index_content:\n",
    "                index_content = [\"# 文章目錄\", \"\"]\n",
    "            all_articles = pd.concat([\n",
    "                df[df['文章編號'].isin(existing_articles)],\n",
    "                new_articles\n",
    "            ]).drop_duplicates(subset=['文章編號'])\n",
    "            all_articles['日期'] = pd.to_datetime(all_articles['日期'])\n",
    "            all_articles = all_articles.sort_values(['作者', '日期'], ascending=[True, False])\n",
    "            for author in sorted(all_articles['作者'].unique()):\n",
    "                index_content.append(f\"## {author}\")\n",
    "                author_articles = all_articles[all_articles['作者'] == author]\n",
    "                for year in sorted(author_articles['日期'].dt.year.unique(), reverse=True):\n",
    "                    index_content.append(f\"\\n### {year}年\")\n",
    "                    year_articles = author_articles[author_articles['日期'].dt.year == year]\n",
    "                    for _, article in year_articles.iterrows():\n",
    "                        title = re.sub(r'[<>:\"/\\\\|?*]', '', article['標題'])[:100]\n",
    "                        file_name = f\"{article['文章編號']}_{title}.md\"\n",
    "                        date_str = article['日期'].strftime('%Y-%m-%d')\n",
    "                        index_content.append(f\"- {date_str} [{article['標題']}](./articles/{file_name})\")\n",
    "                index_content.append(\"\")\n",
    "            with open(index_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(index_content))\n",
    "            self.logger.info(f\"已成功更新文章目錄，新增 {len(new_articles)} 篇文章\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"更新目錄失敗: {str(e)}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"執行文章更新\"\"\"\n",
    "        self.logger.info(\"開始檢查文章更新\")\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                futures = []\n",
    "                max_page = self.get_max_page_number()\n",
    "                self.logger.info(f\"檢測到期刊總頁數: {max_page}\")\n",
    "                article_numbers = set()\n",
    "                for page_no in range(1, max_page + 1):  # 遍歷所有頁面\n",
    "                    self.logger.info(f\"正在檢查第 {page_no} 頁的文章列表\")\n",
    "                    page_articles = self.get_article_urls_from_journal(page_no)\n",
    "                    article_numbers.update(page_articles)\n",
    "                    if not page_articles and page_no > 10:  # 連續10頁無新文章則停止\n",
    "                        self.logger.info(f\"第 {page_no} 頁沒有新文章，停止檢查\")\n",
    "                        break\n",
    "                self.logger.info(f\"找到 {len(article_numbers)} 篇待檢查文章: {sorted(list(article_numbers))}\")\n",
    "                for article_no in article_numbers:\n",
    "                    if str(article_no) not in self.processed_articles:\n",
    "                        futures.append(executor.submit(self.fetch_article, article_no))\n",
    "                with tqdm(total=len(futures), desc=\"更新文章\") as pbar:\n",
    "                    for future in futures:\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result:\n",
    "                                self.save_article(result)\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                fail_count += 1\n",
    "                            pbar.update(1)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"處理文章更新失敗: {str(e)}\")\n",
    "                            fail_count += 1\n",
    "                            pbar.update(1)\n",
    "        finally:\n",
    "            self.logger.info(f\"更新完成：成功 {success_count} 篇，失敗 {fail_count} 篇\")\n",
    "            if success_count > 0:\n",
    "                self.create_index()\n",
    "            self.logger.info(\"更新程序結束\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ArticleScraper(scan_mode=\"recent\")\n",
    "    specific_articles = [913706, 913623, 913646]  # 測試特定文章\n",
    "    for article_no in specific_articles:\n",
    "        scraper.check_specific_article(article_no)\n",
    "    scraper.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
