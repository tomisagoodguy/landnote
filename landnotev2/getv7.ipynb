{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可以更新最近上傳的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 19:07:28,386 - INFO - 已載入 811 篇已處理文章\n",
      "2025-02-02 19:07:28,386 - INFO - 掃描模式: recent\n",
      "2025-02-02 19:07:28,386 - INFO - 日期範圍: 2025-01-03 到 2025-02-02\n",
      "2025-02-02 19:07:28,386 - INFO - 開始爬取文章...\n",
      "2025-02-02 19:07:29,928 - INFO - 第 1 頁找到 0 篇未處理文章\n",
      "2025-02-02 19:07:29,928 - INFO - 爬蟲完成，共處理 0 篇文章\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "import urllib3\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "\n",
    "class ArticleScraper:\n",
    "    def __init__(self, scan_mode=\"recent\", data_file=\"articles.xlsx\"):\n",
    "        \"\"\"初始化爬蟲設定\n",
    "        \n",
    "        Args:\n",
    "            scan_mode (str): 掃描模式 (\"recent\", \"all\", \"range\")\n",
    "            data_file (str): 儲存資料的Excel檔案名稱\n",
    "        \"\"\"\n",
    "        self.SETTINGS = {\n",
    "            'BASE_URL': \"https://real-estate.get.com.tw/Columns/\",\n",
    "            'TARGET_AUTHORS': [\"曾榮耀\", \"許文昌\", \"蘇偉強\"],\n",
    "            'JOURNAL_PARAMS': {\n",
    "                \"no\": \"1282\",\n",
    "                \"pno\": \"51121\"\n",
    "            },\n",
    "            'PERFORMANCE': {\n",
    "                'RETRY_DELAY': 3,\n",
    "                'REQUEST_INTERVAL': 1.5,\n",
    "                'MAX_RETRIES': 5,\n",
    "                'TIMEOUTS': {\n",
    "                    'CONNECT': 10,\n",
    "                    'READ': 30,\n",
    "                    'TOTAL': 40\n",
    "                }\n",
    "            },\n",
    "            'SCAN_MODES': {\n",
    "                'recent': {\n",
    "                    'days': 30,\n",
    "                    'batch_size': 50,\n",
    "                    'max_workers': 4\n",
    "                },\n",
    "                'all': {\n",
    "                    'batch_size': 100,\n",
    "                    'max_workers': 8,\n",
    "                    'article_ranges': [\n",
    "                        {\n",
    "                            \"start\": 900000,\n",
    "                            \"end\": 915000,\n",
    "                            \"description\": \"新年份範圍\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"start\": 409187,\n",
    "                            \"end\": 421516,\n",
    "                            \"description\": \"早期年份範圍\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                'range': {\n",
    "                    'batch_size': 75,\n",
    "                    'max_workers': 6\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 初始化基本屬性\n",
    "        self.scan_mode = scan_mode\n",
    "        if scan_mode not in self.SETTINGS['SCAN_MODES']:\n",
    "            raise ValueError(f\"不支援的掃描模式: {scan_mode}\")\n",
    "\n",
    "        self.mode_settings = self.SETTINGS['SCAN_MODES'][scan_mode]\n",
    "        self.data_file = Path(data_file)\n",
    "\n",
    "        # 初始化URL和參數\n",
    "        self.detail_url = f\"{self.SETTINGS['BASE_URL']}detail.aspx\"\n",
    "        self.journal_url = f\"{self.SETTINGS['BASE_URL']}journal.aspx\"\n",
    "        self.journal_params = self.SETTINGS['JOURNAL_PARAMS']\n",
    "        self.target_authors = self.SETTINGS['TARGET_AUTHORS']\n",
    "\n",
    "        # 初始化效能參數\n",
    "        self.request_interval = self.SETTINGS['PERFORMANCE']['REQUEST_INTERVAL']\n",
    "        self.retry_delay = self.SETTINGS['PERFORMANCE']['RETRY_DELAY']\n",
    "        self.max_retries = self.SETTINGS['PERFORMANCE']['MAX_RETRIES']\n",
    "        self.timeouts = self.SETTINGS['PERFORMANCE']['TIMEOUTS']\n",
    "\n",
    "        # 設定路徑\n",
    "        self.base_dir = Path('data')\n",
    "        self.images_dir = self.base_dir / 'images'\n",
    "        self.logs_dir = self.base_dir / 'logs'\n",
    "        self.failed_image_path = self.base_dir / 'failed.jpg'\n",
    "\n",
    "        # 建立必要的目錄\n",
    "        self._create_directories()\n",
    "\n",
    "        # 設定 logger\n",
    "        self._setup_logger()\n",
    "\n",
    "        # 初始化 session\n",
    "        self.session = self._setup_session()\n",
    "\n",
    "        # 載入已處理的文章\n",
    "        self.processed_articles = self._load_processed_articles()\n",
    "\n",
    "        # 根據掃描模式設定參數\n",
    "        self._setup_scan_mode()\n",
    "\n",
    "        # 設定最後請求時間\n",
    "        self.last_request_time = 0\n",
    "\n",
    "    def _create_directories(self):\n",
    "        \"\"\"創建必要的目錄結構\"\"\"\n",
    "        for directory in [self.base_dir, self.images_dir, self.logs_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 創建預設的失敗圖片\n",
    "        if not self.failed_image_path.exists():\n",
    "            try:\n",
    "                from PIL import Image, ImageDraw\n",
    "                img = Image.new('RGB', (400, 100), color='white')\n",
    "                d = ImageDraw.Draw(img)\n",
    "                d.text((10, 40), \"Image Download Failed\", fill='black')\n",
    "                img.save(self.failed_image_path)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"創建失敗圖片時發生錯誤: {str(e)}\")\n",
    "                self.failed_image_path.touch()\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        \"\"\"設定日誌系統\"\"\"\n",
    "        self.logger = logging.getLogger('ArticleScraper')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # 創建日誌檔案\n",
    "        log_file = self.logs_dir / \\\n",
    "            f\"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "        # 設定處理器\n",
    "        file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "        console_handler = logging.StreamHandler()\n",
    "\n",
    "        # 設定格式\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # 添加處理器\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def _setup_session(self):\n",
    "        \"\"\"設定並返回requests session\"\"\"\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        })\n",
    "        return session\n",
    "\n",
    "    def _load_processed_articles(self):\n",
    "        \"\"\"載入已處理的文章編號集合\"\"\"\n",
    "        processed = set()\n",
    "        if self.data_file.exists():\n",
    "            try:\n",
    "                df = pd.read_excel(self.data_file)\n",
    "                if '文章編號' in df.columns:\n",
    "                    processed = set(df['文章編號'].astype(str))\n",
    "                self.logger.info(f\"已載入 {len(processed)} 篇已處理文章\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"載入已處理文章時發生錯誤: {str(e)}\")\n",
    "        return processed\n",
    "\n",
    "    def _setup_scan_mode(self):\n",
    "        \"\"\"根據掃描模式設定相關參數\"\"\"\n",
    "        self.batch_size = self.mode_settings['batch_size']\n",
    "        self.max_workers = self.mode_settings['max_workers']\n",
    "\n",
    "        now = datetime.now()\n",
    "        if self.scan_mode == 'recent':\n",
    "            self.start_date = now - timedelta(days=self.mode_settings['days'])\n",
    "            self.end_date = now\n",
    "        elif self.scan_mode == 'range':\n",
    "            # 需要外部設定日期範圍\n",
    "            self.start_date = None\n",
    "            self.end_date = None\n",
    "        else:  # 'all' mode\n",
    "            self.start_date = datetime(2016, 1, 1)\n",
    "            self.end_date = now\n",
    "\n",
    "        self.logger.info(f\"掃描模式: {self.scan_mode}\")\n",
    "        if self.start_date and self.end_date:\n",
    "            self.logger.info(f\"日期範圍: {self.start_date.date()} 到 {\n",
    "                             self.end_date.date()}\")\n",
    "\n",
    "    def set_date_range(self, start_date: datetime, end_date: datetime):\n",
    "        \"\"\"設定掃描的日期範圍\"\"\"\n",
    "        if self.scan_mode != 'range':\n",
    "            raise ValueError(\"只有在 'range' 模式下才能設定日期範圍\")\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.logger.info(f\"設定日期範圍: {start_date.date()} 到 {end_date.date()}\")\n",
    "\n",
    "    def wait_between_requests(self):\n",
    "        \"\"\"控制請求間隔\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_request_time\n",
    "        if elapsed < self.request_interval:\n",
    "            time.sleep(self.request_interval - elapsed)\n",
    "        self.last_request_time = time.time()\n",
    "        \n",
    "    def get_max_page_number(self) -> int:\n",
    "        \"\"\"獲取期刊最大頁數\"\"\"\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                self.wait_between_requests()\n",
    "                response = self.session.get(\n",
    "                    self.journal_url,\n",
    "                    params=self.journal_params,\n",
    "                    timeout=self.timeouts['TOTAL'],\n",
    "                    verify=False\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # 找到分頁元素\n",
    "                pagination = soup.select('.pagination a')\n",
    "                if pagination:\n",
    "                    page_numbers = []\n",
    "                    for a in pagination:\n",
    "                        try:\n",
    "                            page_numbers.append(int(a.text.strip()))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                    max_page = max(page_numbers) if page_numbers else 1\n",
    "                    self.logger.info(f\"找到最大頁數: {max_page}\")\n",
    "                    return max_page\n",
    "                return 1\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(\n",
    "                    f\"獲取最大頁數失敗 (嘗試 {retry + 1}/{self.max_retries}): {str(e)}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (retry + 1))\n",
    "                    continue\n",
    "\n",
    "        self.logger.warning(\"無法獲取最大頁數，使用預設值 30\")\n",
    "        return 30\n",
    "\n",
    "    def get_article_urls_from_journal(self, page_no: int) -> List[int]:\n",
    "        \"\"\"從期刊頁面獲取文章編號列表\"\"\"\n",
    "        try:\n",
    "            params = self.journal_params.copy()\n",
    "            params['page_no'] = page_no\n",
    "\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                self.journal_url,\n",
    "                params=params,\n",
    "                timeout=self.timeouts['TOTAL'],\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            articles = []\n",
    "            for link in soup.select('a[href*=\"detail.aspx?no=\"]'):\n",
    "                href = link.get('href', '')\n",
    "                if 'no=' in href:\n",
    "                    article_no = href.split(\n",
    "                        'no=')[-1].split('&')[0]  # 處理可能的額外參數\n",
    "                    try:\n",
    "                        article_no = int(article_no)\n",
    "                        if str(article_no) not in self.processed_articles:\n",
    "                            articles.append(article_no)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "            self.logger.info(f\"第 {page_no} 頁找到 {len(articles)} 篇未處理文章\")\n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"獲取第 {page_no} 頁文章列表失敗: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_article(self, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"抓取單篇文章\"\"\"\n",
    "        if str(article_no) in self.processed_articles:\n",
    "            self.logger.debug(f\"文章 {article_no} 已處理過，跳過\")\n",
    "            return None\n",
    "\n",
    "        for retry in range(self.max_retries):\n",
    "            try:\n",
    "                params = {'no': article_no}\n",
    "                self.wait_between_requests()\n",
    "\n",
    "                response = self.session.get(\n",
    "                    self.detail_url,\n",
    "                    params=params,\n",
    "                    timeout=self.timeouts['TOTAL'],\n",
    "                    verify=False\n",
    "                )\n",
    "\n",
    "                if response.status_code == 404:\n",
    "                    self.logger.info(f\"文章 {article_no} 不存在\")\n",
    "                    return None\n",
    "\n",
    "                response.raise_for_status()\n",
    "                response.encoding = 'utf-8'\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                article_data = self.parse_article(soup, article_no)\n",
    "\n",
    "                if article_data and self.validate_article(article_data):\n",
    "                    self.logger.info(f\"成功抓取文章 {article_no}\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    self.logger.info(f\"文章 {article_no} 不符合條件或解析失敗\")\n",
    "                    return None\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(f\"抓取文章 {article_no} 失敗 (嘗試 {\n",
    "                                  retry + 1}/{self.max_retries}): {str(e)}\")\n",
    "                if retry < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (retry + 1))\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"處理文章 {article_no} 時發生未預期錯誤: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        return None\n",
    "\n",
    "    def parse_article(self, soup: BeautifulSoup, article_no: int) -> Optional[Dict]:\n",
    "        \"\"\"解析文章內容\"\"\"\n",
    "        try:\n",
    "            article_info = {}\n",
    "            author = None\n",
    "\n",
    "            # 解析文章基本信息\n",
    "            for row in soup.select('.columnsDetail_tableRow'):\n",
    "                th = row.select_one('.columnsDetail_tableth')\n",
    "                td = row.select_one('.columnsDetail_tabletd')\n",
    "                if th and td:\n",
    "                    key = th.text.strip()\n",
    "                    value = td.text.strip()\n",
    "                    article_info[key] = value\n",
    "\n",
    "                    if key == '內文':\n",
    "                        article_info['內文HTML'] = str(td)\n",
    "                    elif key == '作者':\n",
    "                        author = value\n",
    "\n",
    "            # 驗證作者\n",
    "            if not author or not any(target in author for target in self.target_authors):\n",
    "                self.logger.debug(f\"文章 {article_no} 作者 {author} 不在目標列表中\")\n",
    "                return None\n",
    "\n",
    "            # 處理文章內容和圖片\n",
    "            content = self.process_content(\n",
    "                article_info.get('內文HTML', ''), article_no)\n",
    "\n",
    "            article_data = {\n",
    "                '文章編號': article_no,\n",
    "                '標題': article_info.get('篇名', ''),\n",
    "                '作者': author,\n",
    "                '日期': article_info.get('日期', ''),\n",
    "                '內文': content,\n",
    "                'URL': f\"{self.detail_url}?no={article_no}\",\n",
    "                '爬取時間': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "\n",
    "            return article_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"解析文章 {article_no} 失敗: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_content(self, html_content: str, article_no: int) -> str:\n",
    "        \"\"\"處理文章內容，包括下載圖片和清理HTML\"\"\"\n",
    "        if not html_content:\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # 處理圖片\n",
    "        for img in soup.find_all('img'):\n",
    "            img_url = img.get('src', '')\n",
    "            if img_url:\n",
    "                local_path = self.download_image(img_url, article_no)\n",
    "                if local_path:\n",
    "                    img['src'] = str(local_path)\n",
    "                else:\n",
    "                    img['src'] = str(self.failed_image_path)\n",
    "                    img['alt'] = '圖片下載失敗'\n",
    "\n",
    "        # 清理HTML並格式化\n",
    "        return self._format_content(soup)\n",
    "\n",
    "    def _format_content(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"格式化文章內容，處理換行和縮排\"\"\"\n",
    "        # 允許的HTML標籤\n",
    "        allowed_tags = {'p', 'br', 'img', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li'}\n",
    "        \n",
    "        # 移除不允許的標籤但保留其文本\n",
    "        for tag in soup.find_all():\n",
    "            if tag.name not in allowed_tags:\n",
    "                tag.unwrap()\n",
    "        \n",
    "        # 處理圖片標籤\n",
    "        for img in soup.find_all('img'):\n",
    "            img.insert_after('\\n[圖片]\\n')  # 在圖片後添加標記\n",
    "        \n",
    "        # 確保段落之間有適當的換行\n",
    "        for p in soup.find_all('p'):\n",
    "            text = p.get_text().strip()\n",
    "            if text:  # 只處理非空段落\n",
    "                p.string = ' '.join(text.split())\n",
    "                p.append('\\n\\n')\n",
    "        \n",
    "        # 處理標題\n",
    "        for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            level = int(h.name[1])  # 獲取標題級別\n",
    "            prefix = '#' * level + ' '  # 根據級別添加 # 符號\n",
    "            h.string = f'\\n{prefix}{h.get_text().strip()}\\n'\n",
    "        \n",
    "        # 處理列表\n",
    "        for li in soup.find_all('li'):\n",
    "            indent = '  '\n",
    "            if li.parent.name == 'ol':\n",
    "                # 為有序列表添加數字\n",
    "                index = len(li.find_previous_siblings('li')) + 1\n",
    "                li.insert(0, f'{indent}{index}. ')\n",
    "            else:\n",
    "                # 為無序列表添加圓點\n",
    "                li.insert(0, f'{indent}• ')\n",
    "            li.append('\\n')\n",
    "        \n",
    "        # 獲取處理後的文本\n",
    "        content = soup.get_text()\n",
    "        \n",
    "        # 清理最終文本\n",
    "        content = re.sub(r'\\n{3,}', '\\n\\n', content)  # 移除過多的空行\n",
    "        content = re.sub(r'[ \\t]+', ' ', content)     # 標準化空格\n",
    "        content = re.sub(r' *\\n *', '\\n', content)    # 清理行首尾空格\n",
    "        \n",
    "        # 分段處理並保持段落間的空行\n",
    "        paragraphs = [p.strip() for p in content.split('\\n') if p.strip()]\n",
    "        formatted_content = '\\n\\n'.join(paragraphs)\n",
    "        \n",
    "        return formatted_content.strip()\n",
    "\n",
    "\n",
    "    \n",
    "    def download_image(self, img_url: str, article_no: int) -> Optional[Path]:\n",
    "        \"\"\"下載圖片並返回本地路徑\"\"\"\n",
    "        try:\n",
    "            # 確保是完整的URL\n",
    "            if not img_url.startswith(('http://', 'https://')):\n",
    "                img_url = urllib.parse.urljoin(\n",
    "                    self.SETTINGS['BASE_URL'], img_url)\n",
    "\n",
    "            # 生成圖片檔名\n",
    "            url_hash = hashlib.md5(img_url.encode()).hexdigest()\n",
    "            img_ext = img_url.split('.')[-1].lower()\n",
    "            if img_ext not in ['jpg', 'jpeg', 'png', 'gif']:\n",
    "                img_ext = 'jpg'\n",
    "\n",
    "            img_filename = f\"{article_no}_{url_hash[:8]}.{img_ext}\"\n",
    "            img_path = self.images_dir / img_filename\n",
    "\n",
    "            # 如果圖片已存在，直接返回路徑\n",
    "            if img_path.exists():\n",
    "                return img_path\n",
    "\n",
    "            # 下載圖片\n",
    "            self.wait_between_requests()\n",
    "            response = self.session.get(\n",
    "                img_url,\n",
    "                timeout=self.timeouts['TOTAL'],\n",
    "                verify=False,\n",
    "                stream=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 驗證內容類型\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            if not content_type.startswith('image/'):\n",
    "                raise ValueError(f\"非圖片內容類型: {content_type}\")\n",
    "\n",
    "            # 保存圖片\n",
    "            with open(img_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "            self.logger.debug(f\"成功下載圖片: {img_filename}\")\n",
    "            return img_path\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"下載圖片失敗 ({img_url}): {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def validate_article(self, article_data: Dict) -> bool:\n",
    "        \"\"\"驗證文章數據是否完整且符合條件\"\"\"\n",
    "        required_fields = ['文章編號', '標題', '作者', '日期', '內文']\n",
    "\n",
    "        # 檢查必要欄位\n",
    "        if not all(field in article_data for field in required_fields):\n",
    "            return False\n",
    "\n",
    "        # 檢查日期格式和範圍\n",
    "        try:\n",
    "            article_date = datetime.strptime(article_data['日期'], \"%Y-%m-%d\")\n",
    "            if self.start_date and article_date < self.start_date:\n",
    "                return False\n",
    "            if self.end_date and article_date > self.end_date:\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "        # 檢查內容長度\n",
    "        if len(article_data['內文']) < 100:  # 最小內容長度要求\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save_articles(self, articles: List[Dict]):\n",
    "        \"\"\"保存文章數據到Excel檔案\"\"\"\n",
    "        try:\n",
    "            # 讀取現有數據\n",
    "            if self.data_file.exists():\n",
    "                df_existing = pd.read_excel(self.data_file)\n",
    "                # 移除重複的文章\n",
    "                new_articles_df = pd.DataFrame(articles)\n",
    "                df = pd.concat([df_existing, new_articles_df],\n",
    "                               ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['文章編號'], keep='last')\n",
    "            else:\n",
    "                df = pd.DataFrame(articles)\n",
    "\n",
    "            # 排序並保存\n",
    "            df = df.sort_values('日期', ascending=False)\n",
    "            df.to_excel(self.data_file, index=False)\n",
    "            self.logger.info(f\"成功保存 {len(articles)} 篇文章到 {self.data_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"保存文章數據失敗: {str(e)}\")\n",
    "            # 備份數據\n",
    "            backup_file = self.base_dir / \\\n",
    "                f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "            pd.DataFrame(articles).to_excel(backup_file, index=False)\n",
    "            self.logger.info(f\"已創建備份文件: {backup_file}\")\n",
    "\n",
    "    def process_batch(self, article_numbers: List[int]) -> List[Dict]:\n",
    "        \"\"\"使用線程池處理一批文章\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self.fetch_article, no)\n",
    "                       for no in article_numbers]\n",
    "            articles = []\n",
    "\n",
    "            for future in tqdm(futures, desc=\"處理文章\", unit=\"篇\"):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        articles.append(result)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"處理文章批次時發生錯誤: {str(e)}\")\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"主要運行方法\"\"\"\n",
    "        self.logger.info(\"開始爬取文章...\")\n",
    "        total_articles = []\n",
    "\n",
    "        try:\n",
    "            if self.scan_mode == 'all':\n",
    "                # 處理所有文章範圍\n",
    "                for range_info in self.mode_settings['article_ranges']:\n",
    "                    start = range_info['start']\n",
    "                    end = range_info['end']\n",
    "                    desc = range_info['description']\n",
    "\n",
    "                    self.logger.info(f\"處理{desc}: {start} 到 {end}\")\n",
    "                    article_numbers = list(range(start, end + 1))\n",
    "\n",
    "                    # 分批處理\n",
    "                    for i in range(0, len(article_numbers), self.batch_size):\n",
    "                        batch = article_numbers[i:i + self.batch_size]\n",
    "                        articles = self.process_batch(batch)\n",
    "                        if articles:\n",
    "                            total_articles.extend(articles)\n",
    "                            self.save_articles(articles)  # 定期保存\n",
    "\n",
    "            else:  # recent 或 range 模式\n",
    "                max_page = self.get_max_page_number()\n",
    "\n",
    "                for page in range(1, max_page + 1):\n",
    "                    article_numbers = self.get_article_urls_from_journal(page)\n",
    "                    if not article_numbers:\n",
    "                        continue\n",
    "\n",
    "                    articles = self.process_batch(article_numbers)\n",
    "                    if articles:\n",
    "                        total_articles.extend(articles)\n",
    "                        self.save_articles(articles)  # 定期保存\n",
    "\n",
    "            # 最終保存\n",
    "            if total_articles:\n",
    "                self.save_articles(total_articles)\n",
    "\n",
    "            self.logger.info(f\"爬蟲完成，共處理 {len(total_articles)} 篇文章\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.info(\"收到中斷信號，正在保存已處理的文章...\")\n",
    "            if total_articles:\n",
    "                self.save_articles(total_articles)\n",
    "            self.logger.info(\"程序已安全停止\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"執行過程中發生錯誤: {str(e)}\")\n",
    "            if total_articles:\n",
    "                self.save_articles(total_articles)\n",
    "            raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 使用示例\n",
    "    scraper = ArticleScraper(scan_mode=\"recent\", data_file=\"articles.xlsx\")\n",
    "\n",
    "    # 如果是range模式，需要設定日期範圍\n",
    "    # scraper.set_date_range(\n",
    "    #     start_date=datetime(2023, 1, 1),\n",
    "    #     end_date=datetime(2023, 12, 31)\n",
    "    # )\n",
    "\n",
    "    scraper.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
